<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Von Neumann Architecture - an overview | ScienceDirect Topics</title><meta name="robots" content="INDEX,FOLLOW,NOARCHIVE,NOODP,NOYDIR"><link rel="canonical" href="https://www.sciencedirect.com/topics/computer-science/von-neumann-architecture" /><meta name="SDTech" content="Proudly brought to you by the SD Technology team in London, Dayton, and Amsterdam">
<link rel='shortcut icon' href='https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/13/images/favSD.ico' type='image/x-icon' />
<link rel='icon' href='https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/13/images/favSD.ico' type='image/x-icon'>
<link rel='stylesheet' href='https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/78869a99bb515d27b0258c9bf222cd4d21e6a37f/style.css'>

<script crossorigin="anonymous" type="b921dd7e5908c3e856e29bb6-text/javascript">
        (function(){
          if(window.BOOMR && window.BOOMR.version){return;}
          var dom,doc,where,iframe = document.createElement('iframe'),win = window;

          function boomerangSaveLoadTime(e) {
            win.BOOMR_onload=(e && e.timeStamp) || new Date().getTime();
          }
          if (win.addEventListener) {
            win.addEventListener("load", boomerangSaveLoadTime, false);
          } else if (win.attachEvent) {
            win.attachEvent("onload", boomerangSaveLoadTime);
          }

          iframe.src = "javascript:false";
          iframe.title = ""; iframe.role="presentation";
          (iframe.frameElement || iframe).style.cssText = "width:0;height:0;border:0;display:none;";
          where = document.getElementsByTagName('script')[0];
          where.parentNode.insertBefore(iframe, where);

          try {
            doc = iframe.contentWindow.document;
          } catch(e) {
            dom = document.domain;
            iframe.src="javascript:var d=document.open();d.domain='"+dom+"';void(0);";
            doc = iframe.contentWindow.document;
          }
          doc.open()._l = function() {
            var js = this.createElement("script");
            if(dom) this.domain = dom;
            js.id = "boomr-if-as";
            js.src = 'https://c.go-mpulse.net/boomerang/2FBN2-NKMGU-EJKY8-ZANKZ-SUJZF';
            js.crossorigin = "anonymous";
            BOOMR_lstart=new Date().getTime();
            this.body.appendChild(js);
          };
          doc.write('<body onload="document._l();">');
          doc.close();
        })();
      </script>

<script id="adobe-dtm" src= https://assets.adobedtm.com/376c5346e33126fdb6b2dbac81e307cbacfd7935/satelliteLib-b7cfe8df39a4e5eec5536bba80e13f4b6fa0dd7c.js type="b921dd7e5908c3e856e29bb6-text/javascript"></script>
</head>
<body>
<!--[if lt IE 9]>
      <div id="ie8Warning" class="warning">
        <script>function ie8click() {
  const node = document.getElementById('ie8Warning');
  document.cookie = 'ie_warning_state=1';
  node.parentNode.removeChild(node);
}</script>
        <p>Please note that Internet Explorer version 8.x is not supported as of January 1, 2016.
        Please refer to <a href="https://service.elsevier.com/app/answers/detail/a_id/9831/supporthub/sciencedirect/">this page</a> for more information.</p>
        <a class="warning-close" onclick="ie8click()" title="Close IE warning">&times;</a>
      </div>
    <![endif]-->
<a class="skip" title="Skip to Main content" href="#main_content">
Skip to Main content
</a>
<div id="react-root" class="react-root"><div data-reactroot=""><div id="header"><div class="u-bg-white" style="overflow:visible" role="banner"><div class="els-header" style="min-height:80px"><a id="els-main-title-link" href="/"><svg id="els-header-wordmark" class="u-fill-orange u-margin-l-top u-margin-s-left u-margin-l-left-from-sm" viewBox="-3334 3439.4 163 26" style="width:163px;height:24px"><title>ScienceDirect</title><g transform="scale(.8125,.8125)"><path d="M-4099.8,4240.4c0-1.8,1-3.6,4.4-3.6c1.7,0,3.7,0.5,5.5,1.6l0.2-3.1c-1.7-0.7-3.5-1.1-5.6-1.1 c-5.5,0-7.8,2.9-7.8,6.4c0,6.6,10.4,7.2,10.4,11.7c0,1.8-1.4,3.6-4.6,3.6c-2,0-4-0.7-5.6-1.6l-0.4,3.1c1.8,0.9,4.2,1.2,6.1,1.2 c5,0,7.9-2.9,7.9-6.5C-4089.4,4245.8-4099.8,4244.9-4099.8,4240.4"></path><path id="c" d="M-4080.3,4242.9c0.3-0.1,0.8-0.3,2-0.3c2,0,2.9,0.4,2.9,1.9h2.8c0-0.4,0-0.9,0-1.3 c-0.3-2.3-2.5-3.2-5.8-3.2c-3.6,0-7.9,2.7-7.9,9.2c0,6.2,3.3,9.4,7.8,9.4c2,0,4.1-0.4,5.9-1.6l-0.2-2.7c-1.2,1-3.4,1.8-4.8,1.8 c-2.5,0-5.4-2-5.4-7C-4083.1,4244.3-4080.6,4243.1-4080.3,4242.9"></path><path id="i" d="M-4068,4233.1c-1.1,0-1.9,1.1-1.9,2.1c0,1.1,0.9,2.2,1.9,2.2s2-1.1,2-2.2 C-4066,4234.1-4067,4233.1-4068,4233.1 M-4069.5,4258.1h3v-17.7h-3V4258.1z"></path><path id="e" d="M-4057.1,4243.1c0.2-0.2,1.5-0.5,2.3-0.5c2.9,0,4.4,0.6,4.6,4.1h-8.8 C-4058.7,4244.4-4057.4,4243.3-4057.1,4243.1z M-4047.5,4248.9c0-6.1-1.7-8.9-7.1-8.9c-4.6,0-7.9,3.3-7.9,9.4 c0,5.8,3.5,9.2,7.9,9.2c3.3,0,5.1-0.7,6.7-1.7l-0.2-2.7c-1.1,0.9-3.5,1.8-5.5,1.8c-3.7,0-5.8-2.3-5.8-6.5v-0.6L-4047.5,4248.9"></path><path d="M-4034.8,4240c-2.6,0-4.3,1.3-5.7,3.1l-0.5-2.6h-2.8l0.2,1.4c0.1,0.9,0.2,2.1,0.2,3.5v12.8h3v-11.9 c0.8-1.1,2.5-2.9,2.9-3.1c0.3-0.2,1.5-0.5,2.5-0.5c2.7,0,2.9,1.4,3,4c0,1.4,0,3.7,0,3.7c0,3.5-0.1,7.6-0.1,7.6h3 c0,0,0.1-5.3,0.1-8.2c0-1.8,0-3.5-0.1-5.3C-4029.5,4241-4031.6,4240-4034.8,4240"></path><path d="M-3982.5,4255.6h-4.4v-18.6h4.8c6.4,0,8.2,5.2,8.2,9.2C-3973.8,4252.2-3976.5,4255.6-3982.5,4255.6z M-3981.6,4234.6h-8.4v23.5h8.1c8.6,0,11.5-6.7,11.5-11.9C-3970.4,4240.9-3973.2,4234.6-3981.6,4234.6"></path><path d="M-3950.5,4240c-1.9,0-3.4,1.7-4,3.2l-0.5-2.8h-2.8l0.2,1.4c0.1,0.9,0.2,2.1,0.2,3.4v12.8h3v-11.2 c0.6-1.5,2-4.4,3.7-4.4c1.1,0,1.2,1.2,1.2,1.4l2.5-0.7v-0.2c0,0,0-0.2-0.1-0.5C-3947.4,4240.9-3948.5,4240-3950.5,4240"></path><path d="M-3903.5,4255.2c-1.1,0.4-2,0.8-3,0.8c-1.4,0-1.9-0.8-1.9-2.8v-10.4h4.6v-2.3h-4.6v-4.7h-2.9v4.7h-3.2v2.3h3.2 v11.4c0,3.1,1.6,4.4,3.9,4.4c1.4,0,3-0.5,4.1-0.9L-3903.5,4255.2"></path><g><path d="M-3923.3,4242.9c0.3-0.1,0.8-0.3,2-0.3c2,0,2.9,0.4,2.9,1.9h2.8c0-0.4,0-0.9,0-1.3 c-0.3-2.3-2.5-3.2-5.8-3.2c-3.6,0-7.9,2.7-7.9,9.2c0,6.2,3.3,9.4,7.8,9.4c2,0,4.1-0.4,5.9-1.6l-0.2-2.7c-1.2,1-3.4,1.8-4.8,1.8 c-2.5,0-5.4-2-5.4-7C-3926.1,4244.3-3923.6,4243.1-3923.3,4242.9"></path></g><g><path d="M-3941.6,4243.1c0.2-0.2,1.5-0.5,2.3-0.5c2.9,0,4.4,0.6,4.6,4.1h-8.8 C-3943.2,4244.4-3941.8,4243.3-3941.6,4243.1z M-3932,4248.9c0-6.1-1.7-8.9-7.1-8.9c-4.6,0-7.9,3.3-7.9,9.4c0,5.8,3.5,9.2,7.9,9.2 c3.3,0,5.1-0.7,6.7-1.7l-0.2-2.7c-1.1,0.9-3.5,1.8-5.5,1.8c-3.7,0-5.8-2.3-5.8-6.5v-0.6L-3932,4248.9"></path></g><g><path d="M-3964.5,4233.1c-1.1,0-1.9,1.1-1.9,2.1c0,1.1,0.9,2.2,1.9,2.2s2-1.1,2-2.2 C-3962.5,4234.1-3963.4,4233.1-3964.5,4233.1 M-3965.9,4258.1h3v-17.7h-3V4258.1z"></path></g><g><path d="M-4004.4,4243.1c0.2-0.2,1.5-0.5,2.3-0.5c2.9,0,4.4,0.6,4.6,4.1h-8.8 C-4006,4244.4-4004.6,4243.3-4004.4,4243.1z M-3994.7,4248.9c0-6.1-1.7-8.9-7.1-8.9c-4.6,0-7.9,3.3-7.9,9.4c0,5.8,3.5,9.2,7.9,9.2 c3.3,0,5.1-0.7,6.7-1.7l-0.2-2.7c-1.1,0.9-3.5,1.8-5.5,1.8c-3.7,0-5.8-2.3-5.8-6.5v-0.6L-3994.7,4248.9"></path></g><g><path d="M-4019.1,4242.9c0.3-0.1,0.8-0.3,2-0.3c2,0,2.9,0.4,2.9,1.9h2.8c0-0.4,0-0.9,0-1.3 c-0.3-2.3-2.5-3.2-5.8-3.2c-3.6,0-7.9,2.7-7.9,9.2c0,6.2,3.3,9.4,7.8,9.4c2,0,4.1-0.4,5.9-1.6l-0.2-2.7c-1.2,1-3.4,1.8-4.8,1.8 c-2.5,0-5.4-2-5.4-7C-4021.8,4244.3-4019.4,4243.1-4019.1,4242.9"></path></g></g></svg></a><div class="move-right u-display-inline-block"><nav class="u-clr-grey8 u-show-from-md"><div class="u-display-inline-block" style="margin-top:29px"><span><span class="u-margin-l-right"><a class="anchor qa-journals-and-books u-margin-l-right anchor-has-inherit-color" href="/browse/journals-and-books" id="els-header-navigation-section-journals-and-books" style="border-bottom:"><span class="anchor-text">Journals &amp; Books</span></a></span><a class="anchor qa-journals-and-books u-margin-l-right anchor-has-inherit-color" href="/user/register?returnURL=%2Ftopics%2Fcomputer-science%2Fvon-neumann-architecture" id="els-header-navigation-section-register"><span class="anchor-text">Register</span></a></span><a class="anchor qa-no-js-fallback-link anchor-has-inherit-color" href="/user/login?returnURL=%2Ftopics%2Fcomputer-science%2Fvon-neumann-architecture" id="els-header-user-sign-in"><span class="anchor-text">Sign in</span><svg focusable="false" viewBox="0 0 54 128" width="10.125" height="24" class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></a><span><a class="anchor u-margin-l-hor help-link anchor-has-inherit-color" href="https://service.elsevier.com/app/answers/detail/a_id/25793/supporthub/sciencedirect/" id="help" title="Help" aria-label="Help" target="_blank"><span class="anchor-text"><svg focusable="false" viewBox="0 0 114 128" style="margin-top:-10px" width="21.375" height="24" class="icon icon-help"><path d="m57 8c-14.7 0-28.5 5.72-38.9 16.1-10.38 10.4-16.1 24.22-16.1 38.9 0 30.32 24.68 55 55 55 14.68 0 28.5-5.72 38.88-16.1 10.4-10.4 16.12-24.2 16.12-38.9 0-30.32-24.68-55-55-55zm0 1e1c24.82 0 45 20.18 45 45 0 12.02-4.68 23.32-13.18 31.82s-19.8 13.18-31.82 13.18c-24.82 0-45-20.18-45-45 0-12.02 4.68-23.32 13.18-31.82s19.8-13.18 31.82-13.18zm-0.14 14c-11.55 0.26-16.86 8.43-16.86 18v2h1e1v-2c0-4.22 2.22-9.66 8-9.24 5.5 0.4 6.32 5.14 5.78 8.14-1.1 6.16-11.78 9.5-11.78 20.5v6.6h1e1v-5.56c0-8.16 11.22-11.52 12-21.7 0.74-9.86-5.56-16.52-16-16.74-0.39-0.01-0.76-0.01-1.14 0zm-4.86 5e1v1e1h1e1v-1e1h-1e1z"></path></svg></span></a></span></div></nav></div><div class="u-hide-from-md move-right"><ul><li style="list-style:none;display:inline-block;margin-top:32px" class="u-margin-s-hor u-margin-l-hor-from-sm"><div class="hamburger-button"><button class="button-link button-link-primary" aria-label="Mobile menu" aria-expanded="false" type="button"><svg viewBox="0 0 40 18" width="40" height="18" y="52"><path d="M0 16h40v2H0zm0-8h40v2H0zm0-8h40v2H0z"></path></svg><span class="button-link-text"></span></button></div></li></ul><div style="bottom:0;left:0;position:fixed;top:0;width:100%;z-index:70;opacity:.8" class="u-bg-grey1 u-display-none"></div><div id="mobile-menu" style="overflow:auto;position:fixed;width:288px;right:0;top:0;z-index:1000;height:100%" aria-label="Mobile menu" class="u-bg-grey7 u-clr-grey1 u-display-none" role="navigation"><div class="u-bg-black panel-s"></div><div class="u-bg-grey8 panel-s"><h3 class="text-xs u-clr-grey4 u-margin-xs-bottom"><span>ScienceDirect Guests</span></h3><ul><li style="list-style:none"><a class="anchor journals-and-books-link u-padding-xs-top anchor-has-inherit-color" href="/browse/journals-and-books" style="border-bottom:" id="mobile-journals-and-books-link"><span class="anchor-text">Journals &amp; Books</span></a></li><li style="list-style:none"><a class="anchor u-padding-xs-top anchor-has-inherit-color" href="/user/register?returnURL=%2Ftopics%2Fcomputer-science%2Fvon-neumann-architecture" id="mobile-register-link"><span class="anchor-text">Register</span></a></li></ul></div><div class="panel-s u-bg-grey7"><ul class="text-xs"></ul><ul class="u-margin-s-top text-s"><li style="list-style:none"><a class="anchor anchor-has-inherit-color" href="/user/login?returnURL=%2Ftopics%2Fcomputer-science%2Fvon-neumann-architecture" id="mobile-sign-in-out-link" rel="nofollow"><span class="anchor-text">Sign In</span></a></li><span><a class="anchor u-padding-xs-top text-s help-link anchor-has-inherit-color" href="https://service.elsevier.com/app/answers/detail/a_id/25793/supporthub/sciencedirect/" id="mobile-help" title="Help" aria-label="Help" target="_blank"><span class="anchor-text">Help</span></a></span></ul></div></div></div></div></div></div><main id="main_content" role="main" class="notpdfdownload"><div class="u-padding-s-ver"></div><section class="topic-definition-wrapper row"><div class="definition-inner padding-resp-m u-dark-theme"><div class="row gutters"><section class="col-sm-24 col-md-24 col-lg-16 col-xl-16 u-padding-m-ver topic-definition"><h1 class="u-font-serif u-text-light alt-xl">Von Neumann Architecture</h1><p></p></section><section class="col-sm-24 col-md-24 col-lg-6 col-xl-6 padding-resp-ver-m related-terms move-right"><h2 class="u-h3">Related terms:</h2><ul class="list-tags u-clr-blue size-l" data-aa-region="aa-tp-related-terms"><li class="list-tags-item"><a class="anchor" href="/topics/computer-science/supercomputer" data-aa-name="Supercomputer"><span class="anchor-text">Supercomputer</span></a></li><li class="list-tags-item"><a class="anchor" href="/topics/computer-science/computer-architecture" data-aa-name="Computer Architecture"><span class="anchor-text">Computer Architecture</span></a></li><li class="list-tags-item"><a class="anchor" href="/topics/computer-science/harvard-architecture" data-aa-name="Harvard Architecture"><span class="anchor-text">Harvard Architecture</span></a></li><li class="list-tags-item"><a class="anchor" href="/topics/computer-science/microprocessor" data-aa-name="Microprocessor"><span class="anchor-text">Microprocessor</span></a></li></ul><div class="index-page-link u-margin-m-top"><a class="anchor anchor-has-colored-icon" href="/topics/index" data-aa-region="aa-tp-view-topic-index" data-aa-name=""><span class="anchor-text">View all Topics</span><svg focusable="false" viewBox="0 0 54 128" width="10.125" height="24" class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></a></div></section></div><div class="hor-line u-show-from-md" style="border-color:#505050"></div><div class="action-buttons u-margin-l-top u-show-from-md"><div class="u-display-inline-block"><button class="button-link u-show-from-md u-margin-m-right action-buttons button-link-primary" data-aa-region="aa-tp-download-topic-PDF" data-aa-name="" type="button"><svg focusable="false" viewBox="0 0 98 128" width="18.375" height="24" class="icon icon-download"><path d="m77.38 56.18l-6.6-7.06-16.78 17.24v-40.36h-1e1v40.34l-17.72-17.24-7.3 7.08 29.2 29.32 29.2-29.32m10.62 17.82v2e1h-78v-2e1h-1e1v3e1h98v-3e1h-1e1"></path></svg><span class="button-link-text">Download as PDF</span></button></div><div class="u-display-inline-block"><button class="button-link u-show-from-md action-buttons button-link-primary" type="button"><svg focusable="false" viewBox="0 0 108 128" width="20.25" height="24" class="icon icon-bell"><path d="m44.76 112h-10.39c1.78 9.18 9.9 16 19.63 16s17.85-6.82 19.63-16h-10.39c-1.49 3.64-5.07 6.21-9.24 6.21s-7.75-2.57-9.24-6.21zm-31.68-18c7.08-7.97 19.68-25.19 21.35-48.16l1.18-16.59c3.2-1.33 9.63-3.45 18.17-3.45 8.53 0 15.71 2.12 18.91 3.46l1.21 16.58c1.68 22.98 14.12 40.2 21.09 48.16zm94.91-1.07l-1.63-1.49c-0.2-0.18-20.48-18.96-22.49-46.33l-1.61-21.91-2.27-1.33c-0.41-0.24-10.96-5.87-26.23-5.87-15.26 0-25.05 5.64-25.46 5.88l-2.26 1.33-1.59 21.9c-1.98 27.35-22.77 46.3-22.77 46.3l-1.68 1.49v11.1h108l-0.01-11.07"></path></svg><span class="button-link-text">Set alert</span></button></div><div class="popover u-show-from-md text-m move-right" id="about-this-page"><div id="popover-trigger-about-this-page"><button class="button-link action-buttons u-padding-s-bottom button-link-primary" role="button" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 100 128" aria-hidden="true" width="18.75" height="24" class="icon icon-info"><path d="m5e1 14c-13.08 0-25.4 5.1-34.64 14.36-9.26 9.24-14.36 21.56-14.36 34.64 0 27.02 21.98 49 49 49 13.1 0 25.4-5.1 34.66-14.36 9.24-9.24 14.34-21.56 14.34-34.64 0-27.02-21.98-49-49-49zm0 9.6c21.72 0 39.4 17.68 39.4 39.4 0 10.52-4.1 20.42-11.54 27.86s-17.34 11.52-27.86 11.52c-21.72 0-39.4-17.66-39.4-39.38 0-10.52 4.1-20.42 11.54-27.86s17.34-11.54 27.86-11.54zm-5 14.4v1e1h1e1v-1e1h-1e1zm0 16v34h1e1v-34h-1e1z"></path></svg><span class="button-link-text">About this page</span></button></div></div></div></div></section><section id="snippets-container" class="snippets-container screen"><div class="snippets-container-inner padding-resp-hor-m u-padding-l-ver"><div class="snippets-actions"><h2 class="u-padding-m-top u-padding-s-bottom u-clr-grey8 move-left">Learn more about Von Neumann Architecture</h2></div><div id="snippets-wrapper"><article class="snippet"><div class="snippet-inner"><h2 class="u-font-serif u-text-light" id="tp-snippet-chp-title-B9780128024591000063"><a class="anchor" href="/science/article/pii/B9780128024591000063" data-aa-region="aa-tp-snippet-chp-title" data-aa-name="Security in embedded systems*" data-hack="#"><span class="anchor-text">Security in embedded systems*</span></a></h2><div class="size-l"><cite><p><span>J. Rosenberg, in </span><a href="/book/9780128024591" class="anchor" data-aa-region="aa-tp-snippet-bk-title" data-aa-name="Rugged Embedded Systems" data-hack="#"><span class="anchor-text">Rugged Embedded Systems</span></a>, 2017</p></cite><section id="B9780128024591000063-s0325"><h3>3.1 Processor Architectures and Security Flaws</h3><p id="B9780128024591000063-p1330">The <span class="topic-highlight">Von Neumann architecture</span>, also known as the Princeton architecture, is a computer architecture based on that described in 1945 by the mathematician and physicist John Von Neumann. He described an architecture for an electronic digital computer with parts consisting of a processing unit containing an arithmetic logic unit (ALU) and processor registers, a control unit containing an instruction register and program counter (PC), a memory to store both data and instructions, external mass storage, and input and output mechanisms. The meaning has evolved to be any stored-program computer in which an instruction fetch and a data operation cannot occur at the same time because they share a common bus.</p><p id="B9780128024591000063-p1335">The design of a Von Neumann architecture is simpler than the more modern Harvard architecture which is also a stored-program system but has one dedicated set of address and data buses for reading data from and writing data to memory, and another set of address and data buses for fetching instructions. A stored-program digital computer is one that keeps its program instructions, as well as its data, in read-write, random-access memory.</p><p id="B9780128024591000063-p1340">A stored-program design also allows for self-modifying code. One early motivation for such a facility was the need for a program to increment or otherwise modify the address portion of instructions, which had to be done manually in early designs. This became less important when index registers and indirect addressing became usual features of machine architecture. Another use was to embed frequently used data in the instruction stream using immediate addressing. Self-modifying code has largely fallen out of favor, since it is usually hard to understand and debug, as well as being inefficient under modern processor pipelining and caching schemes.</p><p id="B9780128024591000063-p1345">On a large scale, the ability to treat instructions as data is what makes assemblers, compilers, linkers, loaders, and other automated programming tools possible. One can “write programs which write programs.” On a smaller scale, repetitive I/O-intensive operations such as the BITBLT image manipulation primitive or pixel &amp; vertex shaders in modern 3D graphics were considered inefficient to run without custom hardware. These operations could be accelerated on general purpose processors with “on the fly compilation” (“just-in-time compilation”) technology, e.g., code-generating programs—one form of self-modifying code that has remained popular.</p><p id="B9780128024591000063-p1350">There are drawbacks to the Von Neumann design especially when it comes to security, which was not even conceived as a problem until the 1980s. Program modifications can be quite harmful, either by accident or design. Since the processor just executes the word the PC points to, there is effectively no distinction between instructions and data. This is precisely the design flaw that attackers use to perform code injection attacks and it leads to the theme of the inherently secure processor: the processor cooperates in security.</p></section></div><p class="u-padding-s-top u-padding-xs-bottom read-full-chapter-link"><a href="/science/article/pii/B9780128024591000063" class="button-alternative button-alternative-primary" aria-describedby="tp-snippet-chp-title-B9780128024591000063" data-aa-region="aa-tp-snippet-read-full-chp" data-aa-name="" data-hack="#"><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-right button-alternative-icon"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg><span class="button-alternative-text">Read full chapter</span></a></p></div><div class="chapter-pdf-download u-show-from-md"><div class="hor-line" style="border-color:#EBEBEB"></div><button class="button-link u-padding-s-ver button-link-primary text-s" data-aa-region="aa-tp-purchase-book" data-aa-name="" type="button"><svg focusable="false" viewBox="0 0 98 128" width="18.375" height="24" class="icon icon-download"><path d="m77.38 56.18l-6.6-7.06-16.78 17.24v-40.36h-1e1v40.34l-17.72-17.24-7.3 7.08 29.2 29.32 29.2-29.32m10.62 17.82v2e1h-78v-2e1h-1e1v3e1h98v-3e1h-1e1"></path></svg><span class="button-link-text">Purchase book</span></button><div class="u-padding-m-bottom"></div></div></article><article class="snippet"><div class="snippet-inner"><h2 class="u-font-serif u-text-light" id="tp-snippet-chp-title-B9780128150719000142"><a class="anchor" href="/science/article/pii/B9780128150719000142" data-aa-region="aa-tp-snippet-chp-title" data-aa-name="Hardware and Software for Digital Signal Processors" data-hack="#"><span class="anchor-text">Hardware and Software for Digital Signal Processors</span></a></h2><div class="size-l"><cite><p><span>Lizhe Tan, Jean Jiang, in </span><a href="/book/9780128150719" class="anchor" data-aa-region="aa-tp-snippet-bk-title" data-aa-name="Digital Signal Processing (Third Edition)" data-hack="#"><span class="anchor-text">Digital Signal Processing (Third Edition)</span></a>, 2019</p></cite><section id="B9780128150719000142-s0120"><h3>14.8 Summary</h3><div><dl class="article-list"><dt>1.</dt><dd><p id="B9780128150719000142-p1735">The <span class="topic-highlight">Von Neumann architecture</span> consists of a single, shared memory for programs and data, a single bus for memory access, an arithmetic unit, and a program control unit. The Von Neumann processor operates fetching and execution cycles seriously.</p></dd><dt>2.</dt><dd><p id="B9780128150719000142-p1740">The Harvard architecture has two separate memory spaces dedicated to program code and to data, respectively, two corresponding address buses, and two data buses for accessing two memory spaces. The Harvard processor offers fetching and executions in parallel.</p></dd><dt>3.</dt><dd><p id="B9780128150719000142-p1745">The DSP special hardware units include an MAC dedicated to DSP filtering operations, a shifter unit for scaling and address generators for circular buffering.</p></dd><dt>4.</dt><dd><p id="B9780128150719000142-p1750">The fixed-point DSP uses integer arithmetic. The data format Q-15 for the fixed-point system is preferred to avoid the overflows.</p></dd><dt>5.</dt><dd><p id="B9780128150719000142-p1755">The floating-point processor uses the floating-point arithmetic. The standard floating-point formats include the IEEE single-precision and double-precision formats.</p></dd><dt>6.</dt><dd><p id="B9780128150719000142-p1760">The architectures and features of fixed-point processors and floating-point processors were briefly reviewed.</p></dd><dt>7.</dt><dd><p id="B9780128150719000142-p1765">Implementing digital filters in the fixed-point DSP system requires scaling filter coefficients so that the filters are in Q-15 format, and input scaling for adder so that overflow during the MAC operations can be avoided.</p></dd><dt>8.</dt><dd><p id="B9780128150719000142-p1770">The floating-point processor is easy to code using the floating-point arithmetic and develop the prototype quickly. However, it is not efficient in terms of the number of instructions it has to complete compared with the fixed-point processor.</p></dd><dt>9.</dt><dd><p id="B9780128150719000142-p1775">The fixed-point processor using fixed-point arithmetic takes much effort to code. But it offers the least number of the instructions for the CPU to execute.</p></dd><dt>10.</dt><dd><p id="B9780128150719000142-p9000">Additional real-time DSP examples are provided, including adaptive filtering, signal quantization and coding, and sample rate conversion.</p></dd></dl></div></section></div><p class="u-padding-s-top u-padding-xs-bottom read-full-chapter-link"><a href="/science/article/pii/B9780128150719000142" class="button-alternative button-alternative-primary" aria-describedby="tp-snippet-chp-title-B9780128150719000142" data-aa-region="aa-tp-snippet-read-full-chp" data-aa-name="" data-hack="#"><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-right button-alternative-icon"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg><span class="button-alternative-text">Read full chapter</span></a></p></div><div class="chapter-pdf-download u-show-from-md"><div class="hor-line" style="border-color:#EBEBEB"></div><button class="button-link u-padding-s-ver button-link-primary text-s" data-aa-region="aa-tp-purchase-book" data-aa-name="" type="button"><svg focusable="false" viewBox="0 0 98 128" width="18.375" height="24" class="icon icon-download"><path d="m77.38 56.18l-6.6-7.06-16.78 17.24v-40.36h-1e1v40.34l-17.72-17.24-7.3 7.08 29.2 29.32 29.2-29.32m10.62 17.82v2e1h-78v-2e1h-1e1v3e1h98v-3e1h-1e1"></path></svg><span class="button-link-text">Purchase book</span></button><div class="u-padding-m-bottom"></div></div></article><article class="snippet"><div class="snippet-inner"><h2 class="u-font-serif u-text-light" id="tp-snippet-chp-title-B9780128498903000034"><a class="anchor" href="/science/article/pii/B9780128498903000034" data-aa-region="aa-tp-snippet-chp-title" data-aa-name="Modern Architectures" data-hack="#"><span class="anchor-text">Modern Architectures</span></a></h2><div class="size-l"><cite><p><span>Bertil Schmidt, ... Moritz Schlarb, in </span><a href="/book/9780128498903" class="anchor" data-aa-region="aa-tp-snippet-bk-title" data-aa-name="Parallel Programming" data-hack="#"><span class="anchor-text">Parallel Programming</span></a>, 2018</p></cite><section id="B9780128498903000034-s0015"><h3>von Neumann Bottleneck</h3><div><p id="B9780128498903000034-p0095">In the classical <em><span class="topic-highlight">von Neumann architecture</span></em> a processor (CPU) is connected to main memory through a bus as shown in <span>Fig. 3.1</span><span id="B9780128498903000034-page48"></span>. In early computer systems timings for accessing main memory and for computation were reasonably well balanced. However, during the past few decades computation speed grew at a much faster rate compared to main memory access speed resulting in a significant performance gap. This discrepancy between CPU compute speed and main memory (DRAM) speed is commonly known as the <strong>von Neumann bottleneck</strong>.</p><figure id="B9780128498903000034-f0010"><div class="download-image"><div class="image-holder"><script src="https://ajax.cloudflare.com/cdn-cgi/scripts/95c75768/cloudflare-static/rocket-loader.min.js" data-cf-settings="b921dd7e5908c3e856e29bb6-|49"></script><img loading="lazy" src="https://ars.els-cdn.com/content/image/3-s2.0-B9780128498903000034-gr001.jpg?_" height="124" alt="" crossorigin="anonymous" onerror="this.onerror=null;window.IMG_NO_CORS=!0;this.removeAttribute('crossorigin');this.src=this.src.replace(/\?.*$/,'')" /></div><div class="u-margin-s-ver text-s"></div></div><div class="captions"><span id="B9780128498903000034-cp0010"><p id="B9780128498903000034-sp0010"><span class="label">Figure 3.1</span>. Basic structure of a classical von Neumann architecture.</p></span></div></figure></div><div><p>Let us demonstrate that with an example. Consider a CPU with eight cores and a clock frequency of 3 GHz. Moreover, assume that 16 double-precision <em>floating-point operations</em> (Flop) can be performed per core in each clock cycle. This results in a peak compute performance of <span class="math"><math><mn>3</mn><mtext>undefined</mtext><mtext>GHz</mtext><mo>×</mo><mn>8</mn><mo>×</mo><mn>16</mn><mo>=</mo><mn>384</mn><mtext>undefined</mtext><mtext>GFlop/s</mtext></math></span>. The CPU is connected to a DRAM module with a peak memory transfer rate of 51.2 GB/s. We want to establish an upper bound on the performance of this system for calculating the dot product of two vectors <em>u</em> and <em>v</em> containing <em>n</em> double-precision numbers stored in main memory:</p><dl class="article-list"><dd><p id="B9780128498903000034-p0105"><span class="monospace">     double dotp = 0.0;</span><br /><span class="monospace">     for (int i = 0; i &lt; n; i++)</span><br /><span class="monospace">        dotp += u[i] * v[i];</span></p></dd></dl><p> We further assume that the length of both vectors <span class="math"><math><mi>n</mi><mo>=</mo><msup><mrow><mn>2</mn></mrow><mrow><mn>30</mn></mrow></msup></math></span> is sufficiently large. Since two operations (one multiplication and one addition) are performed within each iteration of the loop, we have a total of <span class="math"><math><mn>2</mn><mo>⋅</mo><mi>n</mi><mo>=</mo><msup><mrow><mn>2</mn></mrow><mrow><mn>31</mn></mrow></msup></math></span> Flops to compute. Furthermore, the two vectors have to be transferred from main memory to the CPU. Thus, a total volume of <span class="math"><math><msup><mrow><mn>2</mn></mrow><mrow><mn>31</mn></mrow></msup><mo>×</mo><mn>8</mn><mtext> B</mtext><mo>=</mo><mn>16</mn></math></span> GB of data has to be transferred. Based on our system specification, we can determine the minimum amount of time required for computation and data transfer as follows:</p><dl class="article-list"><dt>•</dt><dd><p id="B9780128498903000034-p0110"><strong>Computation:</strong> <span class="math"><math><msub><mrow><mi>t</mi></mrow><mrow><mtext>comp</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mn>2</mn><mtext> GFlops</mtext></mrow><mrow><mn>384</mn><mtext> GFlop/s</mtext></mrow></mfrac><mo>=</mo><mn>5.2</mn><mtext> ms</mtext></math></span></p></dd><dt>•</dt><dd><p id="B9780128498903000034-p0115"><strong>Data transfer:</strong> <span class="math"><math><msub><mrow><mi>t</mi></mrow><mrow><mtext>mem</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mn>16</mn><mtext> GB</mtext></mrow><mrow><mn>51.2</mn><mtext> GB/s</mtext></mrow></mfrac><mo>=</mo><mn>312.5</mn><mtext> ms</mtext></math></span></p></dd></dl><p> If we overlap computation and data transfer, a lower bound on the total execution time can be derived:</p><div class="formula"><span class="label">(3.1)</span><span class="math"><math><msub><mrow><mi>t</mi></mrow><mrow><mtext>exec</mtext></mrow></msub><mo>≥</mo><mi mathvariant="normal">max</mi><mo>⁡</mo><mo stretchy="true" maxsize="2.4ex" minsize="2.4ex">(</mo><msub><mrow><mi>t</mi></mrow><mrow><mtext>comp</mtext></mrow></msub><mo>,</mo><msub><mrow><mi>t</mi></mrow><mrow><mtext>mem</mtext></mrow></msub><mo stretchy="true" maxsize="2.4ex" minsize="2.4ex">)</mo><mo>=</mo><mi mathvariant="normal">max</mi><mo>⁡</mo><mo stretchy="true" maxsize="2.4ex" minsize="2.4ex">(</mo><mn>5.2</mn><mtext> ms</mtext><mo>,</mo><mn>312.5</mn><mtext> ms</mtext><mo stretchy="true" maxsize="2.4ex" minsize="2.4ex">)</mo><mo>=</mo><mn>312.5</mn><mtext> ms</mtext><mspace width="0.2em"></mspace><mo>.</mo></math></span></div><p><span id="B9780128498903000034-page49"></span> The data transfer time clearly dominates. Note that each matrix element is only used once and there is no data re-usage. Thus, dot product computation is <em>memory bound</em>. The corresponding upper bound on the achievable performance can be calculated as <span class="math"><math><mfrac><mrow><msup><mrow><mn>2</mn></mrow><mrow><mn>31</mn></mrow></msup><mtext> Flop</mtext></mrow><mrow><mn>312.5</mn><mtext> ms</mtext></mrow></mfrac><mo>=</mo><mn>6.4</mn></math></span> GFlop/s – meaning that merely less than 2% of the available peak performance can be achieved.</p></div><p id="B9780128498903000034-p0120">In order to overcome the von Neumann bottleneck, computer architects have introduced a number of extensions to the classical architecture. One of those is the addition of a fast memory between CPU and main memory called <strong>cache</strong>. Modern CPUs typically contain a hierarchy of three levels of cache (L1, L2, L3) and current CUDA-enabled GPUs contain two levels. There is the usual trade-off between capacity and speed, e.g. L1-cache is small but fast and the L3-cache is big but slow. In addition caches could be private for a single core or shared between several cores. In the following, we learn more about how caches work and study a number of programs that make effective use of them.</p></section></div><p class="u-padding-s-top u-padding-xs-bottom read-full-chapter-link"><a href="/science/article/pii/B9780128498903000034" class="button-alternative button-alternative-primary" aria-describedby="tp-snippet-chp-title-B9780128498903000034" data-aa-region="aa-tp-snippet-read-full-chp" data-aa-name="" data-hack="#"><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-right button-alternative-icon"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg><span class="button-alternative-text">Read full chapter</span></a></p></div><div class="chapter-pdf-download u-show-from-md"><div class="hor-line" style="border-color:#EBEBEB"></div><button class="button-link u-padding-s-ver button-link-primary text-s" data-aa-region="aa-tp-purchase-book" data-aa-name="" type="button"><svg focusable="false" viewBox="0 0 98 128" width="18.375" height="24" class="icon icon-download"><path d="m77.38 56.18l-6.6-7.06-16.78 17.24v-40.36h-1e1v40.34l-17.72-17.24-7.3 7.08 29.2 29.32 29.2-29.32m10.62 17.82v2e1h-78v-2e1h-1e1v3e1h98v-3e1h-1e1"></path></svg><span class="button-link-text">Purchase book</span></button><div class="u-padding-m-bottom"></div></div></article><article class="snippet"><div class="snippet-inner"><h2 class="u-font-serif u-text-light" id="tp-snippet-chp-title-B9780123742605000026"><a class="anchor" href="/science/article/pii/B9780123742605000026" data-aa-region="aa-tp-snippet-chp-title" data-aa-name="Parallel Hardware and Parallel Software" data-hack="#"><span class="anchor-text">Parallel Hardware and Parallel Software</span></a></h2><div class="size-l"><cite><p><span>Peter S. Pacheco, in </span><a href="/book/9780123742605" class="anchor" data-aa-region="aa-tp-snippet-bk-title" data-aa-name="An Introduction to Parallel Programming" data-hack="#"><span class="anchor-text">An Introduction to Parallel Programming</span></a>, 2011</p></cite><section id="B9780123742605000026-s0015"><h3>2.1.1 The von Neumann architecture</h3><p id="B9780123742605000026-p0025">The “classical” <strong><span class="topic-highlight">von Neumann architecture</span></strong> consists of <strong>main memory</strong>, a <strong>central-processing unit</strong> (CPU) or <strong>processor</strong> or <strong>core</strong>, and an <strong>interconnection</strong> between the memory and the CPU. Main memory consists of a collection of locations, each of which is capable of storing both instructions and data. Every location consists of an address, which is used to access the location and the contents of the location—the instructions or data stored in the location.</p><p id="B9780123742605000026-p0030"><span id="B9780123742605000026-p16"></span>The central processing unit is divided into a control unit and an arithmetic and logic unit (ALU). The control unit is responsible for deciding which instructions in a program should be executed, and the ALU is responsible for executing the actual instructions. Data in the CPU and information about the state of an executing program are stored in special, very fast storage called <strong>registers</strong>. The control unit has a special register called the <strong>program counter</strong>. It stores the address of the next instruction to be executed.</p><div><p id="B9780123742605000026-p0035">Instructions and data are transferred between the CPU and memory via the interconnect. This has traditionally been a <strong>bus</strong>, which consists of a collection of parallel wires and some hardware controlling access to the wires. A von Neumann machine executes a single instruction at a time, and each instruction operates on only a few pieces of data. See <span>Figure 2.1</span>.</p><figure id="B9780123742605000026-f0010"><div class="download-image"><div class="image-holder"><script src="https://ajax.cloudflare.com/cdn-cgi/scripts/95c75768/cloudflare-static/rocket-loader.min.js" data-cf-settings="b921dd7e5908c3e856e29bb6-|49"></script><img loading="lazy" src="https://ars.els-cdn.com/content/image/3-s2.0-B9780123742605000026-f02-01-9780123742605.jpg?_" height="429" alt="" crossorigin="anonymous" onerror="this.onerror=null;window.IMG_NO_CORS=!0;this.removeAttribute('crossorigin');this.src=this.src.replace(/\?.*$/,'')" /></div><div class="u-margin-s-ver text-s"></div></div><div class="captions"><span><p id="B9780123742605000026-sp0010"><span class="label">Figure 2.1</span>. The von Neumann architecture</p></span></div></figure></div><p id="B9780123742605000026-p0040">When data or instructions are transferred from memory to the CPU, we sometimes say the data or instructions are <strong>fetched</strong> or <strong>read</strong> from memory. When data are transferred from the CPU to memory, we sometimes say the data are <strong>written to memory</strong> or <strong>stored</strong>. The separation of memory and CPU is often called the <strong>von Neumann bottleneck</strong>, since the interconnect determines the rate at which instructions and data can be accessed. The potentially vast quantity of data and instructions needed to run a program is effectively isolated from the CPU. In 2010 CPUs are capable of executing instructions more than one hundred times faster than they can fetch items from main memory.<span id="B9780123742605000026-p17"></span></p><p id="B9780123742605000026-p0045">In order to better understand this problem, imagine that a large company has a single factory (the CPU) in one town and a single warehouse (main memory) in another. Further imagine that there is a single two-lane road joining the warehouse and the factory. All the raw materials used in manufacturing the products are stored in the warehouse. Also, all the finished products are stored in the warehouse before being shipped to customers. If the rate at which products can be manufactured is much larger than the rate at which raw materials and finished products can be transported, then it&#x27;s likely that there will be a huge traffic jam on the road, and the employees and machinery in the factory will either be idle for extended periods or they will have to reduce the rate at which they produce finished products.</p><p id="B9780123742605000026-p0050">In order to address the von Neumann bottleneck, and, more generally, improve CPU performance, computer engineers and computer scientists have experimented with many modifications to the basic von Neumann architecture. Before discussing some of these modifications, let&#x27;s first take a moment to discuss some aspects of the software that are used in both von Neumann systems and more modern systems.</p></section></div><p class="u-padding-s-top u-padding-xs-bottom read-full-chapter-link"><a href="/science/article/pii/B9780123742605000026" class="button-alternative button-alternative-primary" aria-describedby="tp-snippet-chp-title-B9780123742605000026" data-aa-region="aa-tp-snippet-read-full-chp" data-aa-name="" data-hack="#"><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-right button-alternative-icon"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg><span class="button-alternative-text">Read full chapter</span></a></p></div><div class="chapter-pdf-download u-show-from-md"><div class="hor-line" style="border-color:#EBEBEB"></div><button class="button-link u-padding-s-ver button-link-primary text-s" data-aa-region="aa-tp-purchase-book" data-aa-name="" type="button"><svg focusable="false" viewBox="0 0 98 128" width="18.375" height="24" class="icon icon-download"><path d="m77.38 56.18l-6.6-7.06-16.78 17.24v-40.36h-1e1v40.34l-17.72-17.24-7.3 7.08 29.2 29.32 29.2-29.32m10.62 17.82v2e1h-78v-2e1h-1e1v3e1h98v-3e1h-1e1"></path></svg><span class="button-link-text">Purchase book</span></button><div class="u-padding-m-bottom"></div></div></article><article class="snippet"><div class="snippet-inner"><h2 class="u-font-serif u-text-light" id="tp-snippet-chp-title-B9780750689762000080"><a class="anchor" href="/science/article/pii/B9780750689762000080" data-aa-region="aa-tp-snippet-chp-title" data-aa-name="Digital Signal Processors" data-hack="#"><span class="anchor-text">Digital Signal Processors</span></a></h2><div class="size-l"><cite><p><span>James D. Broesch, in </span><a href="/book/9780750689762" class="anchor" data-aa-region="aa-tp-snippet-bk-title" data-aa-name="Digital Signal Processing" data-hack="#"><span class="anchor-text">Digital Signal Processing</span></a>, 2009</p></cite><section id="B9780750689762000080-s0110"><h3>Conventional Microprocessors</h3><div><p id="B9780750689762000080-p0310">A conventional microprocessor commonly uses a <em>von Neumann</em> architecture, which means that there is only one common system bus used for transfer of both instructions and data between the external memory chips and the processor (see <span>Figure 8.2</span>). The system bus consists of the three sub-buses: the data bus,<span id="B9780750689762000080-p142"></span> the address bus and the control bus. In many cases, the same system bus is also used for I/O operations. In signal processing applications, this single bus is a bottleneck. Execution of the 10-tap FIR filter (<span>Equation 8.2</span>) will, for instance, require at least 60 bus cycles for instruction fetches and 40 bus cycles for data and coefficient transfers, a total of approximately 100 bus cycles. Hence, even if we are using a fast processor, the speed of the bus cycle will be a limiting factor.</p><figure id="B9780750689762000080-f0020"><div class="download-image"><div class="image-holder"><script src="https://ajax.cloudflare.com/cdn-cgi/scripts/95c75768/cloudflare-static/rocket-loader.min.js" data-cf-settings="b921dd7e5908c3e856e29bb6-|49"></script><img loading="lazy" src="https://ars.els-cdn.com/content/image/3-s2.0-B9780750689762000080-gr2.jpg?_" height="164" alt="" crossorigin="anonymous" onerror="this.onerror=null;window.IMG_NO_CORS=!0;this.removeAttribute('crossorigin');this.src=this.src.replace(/\?.*$/,'')" /></div><div class="u-margin-s-ver text-s"></div></div><div class="captions"><span><p id="B9780750689762000080-sp0020"><span class="label">Figure 8.2</span>. von Neumann architecture, program code and data share memory</p></span></div></figure></div><p id="B9780750689762000080-p0320">One way to ease this problem is the introduction of <em>pipelining</em> techniques, which means that an <em>execution unit</em> (EU) and a <em>bus unit</em> (BU) on the processor chip work simultaneously. While one instruction is being executed in the EU the next instruction is fetched from memory by the BU and put into an instruction queue, feeding the instruction decoder. In this way, idle bus cycles are eliminated. If a jump instruction occurs in the program, a restart of the instruction queue has however to be performed, causing a delay.</p><p id="B9780750689762000080-p0330">Yet another improvement is to add a <em>cache memory</em> on the processor chip. A limited block (some thousand words) of the program code is read into the fast internal cache memory. In this way, instructions can be fetched from the internal cache memory at the same time as data is transferred over the external system bus. This approach may be very efficient in signal processing applications, since in many cases the entire program may fit in the cache, and no reloading is needed.</p><p id="B9780750689762000080-p0340">The execution unit in a conventional microprocessor may consist of an arithmetic logic unit (ALU), a multiplier, a shifter, a floating-point unit (FPU) and some data and flag registers. The ALU commonly handles 2&#x27;s complement arithmetic, and the FPU uses some standard Institute of Electrical and Electronics Engineers (IEEE) floating-point formats. The <em>binary fractions</em> format discussed later in this chapter is often used in signal processing applications but is not supported by general-purpose microprocessors.</p><p id="B9780750689762000080-p0350">Besides program counter (PC) and stack pointer (SP), the <em>address unit</em> (AU) of a conventional microprocessor may contain a number of address and <em>segment</em> registers. There may also be an ALU for calculating addresses used in complicated addressing modes and/or handling virtual memory functions.</p><p id="B9780750689762000080-p0360">The instruction repertoire of many general-purpose microprocessors supports quite exotic addressing modes which are seldom used in signal processing algorithms. On the other hand, instructions for handling such things like <em>delay lines</em> or <em>circular buffers</em> in an efficient manner are rare. The MAC operation often requires a number of computer instructions, and loop counters have to be implemented in software, using general-purpose data registers.</p><p id="B9780750689762000080-p0370">Further, instructions aimed for operating systems and multi-task handling may be found among “higher end” processors. These instructions often are of very limited interest in signal processing applications.</p><p id="B9780750689762000080-p0380">Most of the common processors today are of the <em>complex instruction set computer</em> (CISC) type, i.e. instructions may occupy more than one memory word and hence require more than 1 bus cycle to fetch. Further, these instructions often require more than 1 machine cycle to execute. In many cases,<span id="B9780750689762000080-p143"></span> <em>reduced instruction set computer</em> (RlSC)-type processors may perform better in signal-processing applications.</p><section id="B9780750689762000080-s0130"><h4>Technology Trade-offs</h4><p id="B9780750689762000080-p0390">In a RISC processor, no instruction occupies more than one memory word; it can be fetched in 1 bus cycle and executes in 1 machine cycle. On the other hand, many RISC instructions may be needed to perform the same function as one CISC-type instruction, but in the RISC case, you can get the required complexity only when needed.</p><p id="B9780750689762000080-p0400">Getting analog signals into and out of a general-purpose microprocessor often requires a lot of external hardware. Some microcontrollers have built-in A/D and D/A converters, but in most cases, these converters only have 8- or 12-bit resolution, which is not sufficient in many applications. Sometimes these converters are also quite slow. Even if there are good built-in converters, there is always need for external sample-and-hold (S/H) circuits, and (analog) anti-aliasing and reconstruction filters.</p><p id="B9780750689762000080-p0410">Some microprocessors have built-in high-speed serial communication circuitry, serial peripheral interface (SPI) or I<sup>2</sup>C™. In such cases we still need to have external converters, but the interface will be easier than using the traditional approach, i.e., to connect the converters in parallel to the system bus. Parallel communication will of course be faster, but the circuits needed will be more complicated and we will be stealing capacity from a common, single system bus.</p><p id="B9780750689762000080-p0420">The interrupt facilities found on many general-purpose processors are in many cases “overkill” for signal processing systems. In this kind of real-time application, timing is crucial and <em>synchronous programming</em> is preferred. The number of <em>asynchronous events</em>, e.g., interrupts, is kept to a minimum. Digital signal processing systems using more than a few interrupt sources are rare. One single interrupt source (be it timing or sample rate) or none is common.</p></section></section></div><p class="u-padding-s-top u-padding-xs-bottom read-full-chapter-link"><a href="/science/article/pii/B9780750689762000080" class="button-alternative button-alternative-primary" aria-describedby="tp-snippet-chp-title-B9780750689762000080" data-aa-region="aa-tp-snippet-read-full-chp" data-aa-name="" data-hack="#"><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-right button-alternative-icon"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg><span class="button-alternative-text">Read full chapter</span></a></p></div><div class="chapter-pdf-download u-show-from-md"><div class="hor-line" style="border-color:#EBEBEB"></div><button class="button-link u-padding-s-ver button-link-primary text-s" data-aa-region="aa-tp-purchase-book" data-aa-name="" type="button"><svg focusable="false" viewBox="0 0 98 128" width="18.375" height="24" class="icon icon-download"><path d="m77.38 56.18l-6.6-7.06-16.78 17.24v-40.36h-1e1v40.34l-17.72-17.24-7.3 7.08 29.2 29.32 29.2-29.32m10.62 17.82v2e1h-78v-2e1h-1e1v3e1h98v-3e1h-1e1"></path></svg><span class="button-link-text">Purchase book</span></button><div class="u-padding-m-bottom"></div></div></article><article class="snippet"><div class="snippet-inner"><h2 class="u-font-serif u-text-light" id="tp-snippet-chp-title-B9780128053874000029"><a class="anchor" href="/science/article/pii/B9780128053874000029" data-aa-region="aa-tp-snippet-chp-title" data-aa-name="Instruction Sets" data-hack="#"><span class="anchor-text">Instruction Sets</span></a></h2><div class="size-l"><cite><p><span>Marilyn Wolf, in </span><a href="/book/9780128053874" class="anchor" data-aa-region="aa-tp-snippet-bk-title" data-aa-name="Computers as Components (Fourth Edition)" data-hack="#"><span class="anchor-text">Computers as Components (Fourth Edition)</span></a>, 2017</p></cite><section id="B9780128053874000029-s0120"><h3>What we learned</h3><div><dl class="article-list"><dt>•</dt><dd><p id="B9780128053874000029-p1520">Both the von Neumann and Harvard architectures are in common use today.</p></dd><dt>•</dt><dd><p id="B9780128053874000029-p1525">The programming model is a description of the architecture relevant to instruction operation.</p></dd><dt>•</dt><dd><p id="B9780128053874000029-p1530">ARM is a load-store architecture. It provides a few relatively complex instructions, such as saving and restoring multiple registers.</p></dd><dt>•</dt><dd><p id="B9780128053874000029-p1535">The PIC16F is a very small, efficient microcontroller.</p></dd><dt>•</dt><dd><p id="B9780128053874000029-p1540">The C55x provides a number of architectural features to support the arithmetic loops that are common on digital signal processing code.</p></dd><dt>•</dt><dd><p id="B9780128053874000029-p1545">The C64x organizes instructions into execution packets to enable parallel execution.</p></dd></dl></div></section></div><p class="u-padding-s-top u-padding-xs-bottom read-full-chapter-link"><a href="/science/article/pii/B9780128053874000029" class="button-alternative button-alternative-primary" aria-describedby="tp-snippet-chp-title-B9780128053874000029" data-aa-region="aa-tp-snippet-read-full-chp" data-aa-name="" data-hack="#"><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-right button-alternative-icon"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg><span class="button-alternative-text">Read full chapter</span></a></p></div><div class="chapter-pdf-download u-show-from-md"><div class="hor-line" style="border-color:#EBEBEB"></div><button class="button-link u-padding-s-ver button-link-primary text-s" data-aa-region="aa-tp-purchase-book" data-aa-name="" type="button"><svg focusable="false" viewBox="0 0 98 128" width="18.375" height="24" class="icon icon-download"><path d="m77.38 56.18l-6.6-7.06-16.78 17.24v-40.36h-1e1v40.34l-17.72-17.24-7.3 7.08 29.2 29.32 29.2-29.32m10.62 17.82v2e1h-78v-2e1h-1e1v3e1h98v-3e1h-1e1"></path></svg><span class="button-link-text">Purchase book</span></button><div class="u-padding-m-bottom"></div></div></article><article class="snippet"><div class="snippet-inner"><h2 class="u-font-serif u-text-light" id="tp-snippet-chp-title-B9780123725127000080"><a class="anchor" href="/science/article/pii/B9780123725127000080" data-aa-region="aa-tp-snippet-chp-title" data-aa-name="External Search" data-hack="#"><span class="anchor-text">External Search</span></a></h2><div class="size-l"><cite><p><span>Stefan Edelkamp, Stefan Schrödl, in </span><a href="/book/9780123725127" class="anchor" data-aa-region="aa-tp-snippet-bk-title" data-aa-name="Heuristic Search" data-hack="#"><span class="anchor-text">Heuristic Search</span></a>, 2012</p></cite><section id="B9780123725127000080-s0020"><h3>8.3 Model of Computation</h3><div><p id="B9780123725127000080-p0080"><span>Recent developments of hardware significantly deviate from the <span class="topic-highlight">von Neumann architecture</span>; for example, the next generation of processors has multicore processors and several processor cache levels (see </span><span>Fig. 8.1</span>). Consequences like <em>cache anomalies</em> are well known; for example, recursive programs like <span class="small-caps">Quicksort</span> perform unexpectedly well in practice when compared to other theoretically stronger sorting algorithms.</p><figure id="B9780123725127000080-f0010"><div class="download-image"><div class="image-holder"><script src="https://ajax.cloudflare.com/cdn-cgi/scripts/95c75768/cloudflare-static/rocket-loader.min.js" data-cf-settings="b921dd7e5908c3e856e29bb6-|49"></script><img loading="lazy" src="https://ars.els-cdn.com/content/image/3-s2.0-B9780123725127000080-f08-01-9780123725127.jpg?_" height="392" alt="" crossorigin="anonymous" onerror="this.onerror=null;window.IMG_NO_CORS=!0;this.removeAttribute('crossorigin');this.src=this.src.replace(/\?.*$/,'')" /></div><div class="u-margin-s-ver text-s"></div></div><div class="captions"><span><p id="B9780123725127000080-sp0010"><span class="label">Figure 8.1</span>. The memory hierarchy.</p></span></div></figure></div><div><p id="B9780123725127000080-p0085">The commonly used model for comparing the performances of external algorithms consists of a single processor, small internal memory that can hold up to <em>M</em> data items, and unlimited secondary memory. The size of the input problem (in terms of the number of records) is abbreviated by <em>N</em>. Moreover, the <em>block size B</em> governs the bandwidth of memory transfers. It is often convenient to refer to these parameters in terms of blocks, so we define <span class="math"><math><mi>m</mi><mo>=</mo><mi>M</mi><mo>∕</mo><mi>B</mi></math></span> and <span class="math"><math><mi>n</mi><mo>=</mo><mi>N</mi><mo>∕</mo><mi>B</mi></math></span>. It is usually assumed that at the beginning of the algorithm, the input data is stored in contiguous blocks on external memory, and the same must hold for the output. Only the number of block read and writes are counted, and computations in internal memory do not incur any cost (see <span>Fig. 8.2</span>). An extension of the model considers <em>D</em> disks that can be accessed simultaneously. When using disks in parallel, the technique of <em>disk striping</em> can be employed to essentially increase the block size by a factor of <em>D</em>. Successive blocks are <span id="B9780123725127000080-p322"></span><span id="B9780123725127000080-p323"></span>distributed across different disks. Formally, this means that if we enumerate the records from zero, the <em>i</em> th block of the <em>j</em> th disk contains record number <span class="math"><math><mrow><mo>(</mo><mrow><mi>i</mi><mi>D</mi><mi>B</mi><mo>+</mo><mi>j</mi><mi>B</mi></mrow><mo>)</mo></mrow></math></span> through <span class="math"><math><mrow><mo>(</mo><mrow><mi>i</mi><mi>D</mi><mi>B</mi><mo>+</mo><mrow><mo>(</mo><mrow><mspace width="0.1em"></mspace><mi>j</mi><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow><mi>B</mi><mo>−</mo><mn>1</mn></mrow><mo>)</mo></mrow></math></span>. Usually, it is assumed that <span class="math"><math><mi>M</mi><mo><</mo><mi>N</mi></math></span> and <span class="math"><math><mi>D</mi><mi>B</mi><mo><</mo><mi>M</mi><mo>∕</mo><mn>2</mn></math></span>.</p><figure id="B9780123725127000080-f0015"><div class="download-image"><div class="image-holder"><script src="https://ajax.cloudflare.com/cdn-cgi/scripts/95c75768/cloudflare-static/rocket-loader.min.js" data-cf-settings="b921dd7e5908c3e856e29bb6-|49"></script><img loading="lazy" src="https://ars.els-cdn.com/content/image/3-s2.0-B9780123725127000080-f08-02-9780123725127.jpg?_" height="242" alt="" crossorigin="anonymous" onerror="this.onerror=null;window.IMG_NO_CORS=!0;this.removeAttribute('crossorigin');this.src=this.src.replace(/\?.*$/,'')" /></div><div class="u-margin-s-ver text-s"></div></div><div class="captions"><span><p id="B9780123725127000080-sp0015"><span class="label">Figure 8.2</span>. The external memory model.</p></span></div></figure></div><p id="B9780123725127000080-p0090">We distinguish two general approaches of external memory algorithms: either we can devise algorithms to solve specific computational problems while explicitly controlling secondary memory access, or we can develop general-purpose <em>external memory data structures</em>, such as stacks, queues, search trees, priority queues, and so on, and then use them in algorithms that are similar to their internal memory counterparts.</p></section></div><p class="u-padding-s-top u-padding-xs-bottom read-full-chapter-link"><a href="/science/article/pii/B9780123725127000080" class="button-alternative button-alternative-primary" aria-describedby="tp-snippet-chp-title-B9780123725127000080" data-aa-region="aa-tp-snippet-read-full-chp" data-aa-name="" data-hack="#"><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-right button-alternative-icon"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg><span class="button-alternative-text">Read full chapter</span></a></p></div><div class="chapter-pdf-download u-show-from-md"><div class="hor-line" style="border-color:#EBEBEB"></div><button class="button-link u-padding-s-ver button-link-primary text-s" data-aa-region="aa-tp-purchase-book" data-aa-name="" type="button"><svg focusable="false" viewBox="0 0 98 128" width="18.375" height="24" class="icon icon-download"><path d="m77.38 56.18l-6.6-7.06-16.78 17.24v-40.36h-1e1v40.34l-17.72-17.24-7.3 7.08 29.2 29.32 29.2-29.32m10.62 17.82v2e1h-78v-2e1h-1e1v3e1h98v-3e1h-1e1"></path></svg><span class="button-link-text">Purchase book</span></button><div class="u-padding-m-bottom"></div></div></article><article class="snippet"><div class="snippet-inner"><h2 class="u-font-serif u-text-light" id="tp-snippet-chp-title-B9780124080898000033"><a class="anchor" href="/science/article/pii/B9780124080898000033" data-aa-region="aa-tp-snippet-chp-title" data-aa-name="Advances in Computers" data-hack="#"><span class="anchor-text">Advances in Computers</span></a></h2><div class="size-l"><cite><p><span>Amjad Ali, Khalid Saifullah Syed, in </span><a href="/science/bookseries/00652458" class="anchor" data-aa-region="aa-tp-snippet-bk-title" data-aa-name="Advances in Computers" data-hack="#"><span class="anchor-text">Advances in Computers</span></a>, 2013</p></cite><section id="B9780124080898000033-s0010"><h3>2 Modern Microprocessor Based Computer Systems</h3><p id="B9780124080898000033-p0025">The basic physical organization of a modern computer, based on the <em><span class="topic-highlight">von Neumann architecture</span></em> model, comprises 5 units, namely memory, control, arithmetic-&amp;-logic, input, and output. The <em>central processing unit</em> (CPU) comprises control and arithmetic-&amp;-logic units. The functioning of a computer is precisely the execution of the <em>instructions</em> to process the <em>data</em> by its CPU. The instructions are the primitive operations that the CPU may execute, such as moving the contents of a memory location (called as <em>register</em>) to another memory location within the CPU, or adding the contents of two CPU registers. The control unit fetches the data/instruction from the <em>system memory</em> or <em>main memory</em>, sometimes also referred to as the <em>random access memory</em> (RAM). The data is then processed by the arithmetic-&amp;-logic unit, <strong>sequentially</strong>, according to the instructions decoded by the control unit. Storing both the data and the instructions in a single main memory unit is an essential feature of the von-Neumann architecture. The input and output units provide interface between computer and the human.</p><p id="B9780124080898000033-p0030">Not only the CPU, but also the memory system plays a crucial role in determining the overall computational performance of the computer. The memory system of a modern computer is complicated one. A number of smaller and faster memory units, called <em>cache memories</em> or simply <em>caches</em>, are placed between the CPU and the main memory. The idea of a cache memory is to bring only some part of the program data needed currently from main memory into the cache to speedup the data access by the CPU. The cache memories form a memory hierarchy consisting of a number of levels <span id="B9780124080898000033-p91"></span>in view of their distance from the CPU. The <em>access time</em> and <em>size</em> of the data increase as the hierarchy level gets away from the CPU. The memory hierarchy (combining smaller and faster caches with larger, slower, and cheaper main memory) behaves most of the time like a fast and large memory. This is mainly due to the fact that the caches are to exploit the feature of <em>locality</em> of memory references, also called the <em>principle of locality</em>, which is often exhibited by the computer programs. Common types of the locality of reference include the <em>spatial</em> locality (local in space) and the <em>temporal</em> locality (local in time). Spatial locality of reference occurs when a program accesses the data that is stored contiguously (for example, elements of an array) within a short period of time. Caches are used to exploit this feature of spatial locality by pre-fetching from the main memory some data contiguous to the requested one, into a cache. Temporal locality of reference occurs when a program accesses a used data item again after a short period of time (for example, in a loop). Caches are used to exploit this feature of temporal locality by retaining recently used data into a cache for a certain period of time. Note that the locality of reference is a property of the computer programs but is exploited in the memory system design through the caches. This, definitely, indicates that during the coding a programmer should take care to develop the code so as to enhance both the types of localities of reference for efficient cache utilization. This could be achieved by coding in a way that the data is accessed in a sequential/contiguous fashion and, if required to be reused, is accessed again as soon as possible.</p><p id="B9780124080898000033-p0035">A modern CPU (microprocessor) executes (at least) one instruction per clock cycle. Each different type of CPU architecture has its unique set of instructions, called its <em>instruction set architecture</em> (ISA). The instruction set architecture of a computer can be thought of the language that the computer can understand. Based on the type of ISA, there are two important classes of modern (microprocessor based) computer architectures: <em>CISC</em> (Complex Instruction Set Computer) architecture and <em>RISC</em> (Reduced Instruction Set Computer) architecture. The basic CISC architecture is essentially the von Neumann architecture in the sense of storing both the instruction and the data inside a common memory unit. On the other hand, the basic RISC architecture has two entirely separate memory spaces for the instructions and the data, which is the feature that was first introduced in Harvard architecture to overcome the bottleneck in the von Neumann architecture due to data-instruction shared paths between the CPU and the memory. CISC philosophy is that the ISA has a large number of instructions (and addressing modes, as well) with varying number of required clock cycles and execution time. <span id="B9780124080898000033-p92"></span>Also certain instructions can perform multiple primitive operations. RISC philosophy is that the ISA has a small number of primitive instructions for ease in hardware manufacturing and thus the complicated operations are performed, at program level, by combining simpler ones. Due to its very nature, RISC architecture is usually experienced to be faster and efficient than a comparable CISC architecture. However, due to continuing quest for enhancement and flexibility, today a CPU executing an ISA based on CISC may exhibit certain characteristics of RISC and vice versa. Thus, the features of CISC and RISC architectures have been morphing with each other. Classic CISC architecture examples include VAX (by DEC), PDP-11 (by DEC), Motorola 68000 (by Freescale/Motorola), and x86 (mainly by Intel). The modern CISC architecture, x86-64, based processors like Pentium (by Intel) and Athlon (by AMD) basically evolved from the classic CISC architecture x86, but they exhibit several RISC features. Currently, Xeon (by Intel) and Opteron (by AMD) are the two quite prominent market icons based on x86-64 architecture. Famous RISC architecture examples include MIPS (by MIPS Technologies), POWER (mainly by IBM), SPARC (mainly by SUN/Oracle), ALPHA (by DEC), and ARM for embedded systems (by ARM Ltd.).</p><p id="B9780124080898000033-p0040">Today, Intel and AMD are two major vendors in the microprocessor industry, each with their own line of CPU architectures. The x86-64 CPUs from Intel and AMD, basically emerged as CISC architectures, now incorporate a number of RISC features, especially to provide for <strong>Instructions Level Parallelism</strong>—<strong>ILP</strong> (details later on). Interestingly, today the microprocessors (from Intel and AMD) implement the RISC feature of separate memory space for the data and the instructions (for Level-1 cache, at least).</p><p id="B9780124080898000033-p0045">Another main specialty of a modern CPU is that a number of CPU cores are fused together on a single chip/die with a common integrated memory controller for all the cores. Initially, dual core CPU chips were introduced around the year 2005 but, as of the year 2013, 12/16-core CPU chips are commonly available in the market, although the price might get manifold with linear increase in the number of cores per chip. Moreover, getting the best performance out of a larger number of cores in a single CPU chip is currently a challenging task, mainly due to the memory bandwidth limitations. A multicore CPU provides for more clock cycles by summing the clock cycles contributed by each of its cores. Thus, it is keeping the well-known <em>Moore’s law</em> effective, even today, to some extent. In fact, they provide for tackling the issues of high power requirements and heat dissipation realized in the case when all the cores are there in separate CPU chips, instead of <span id="B9780124080898000033-p93"></span>being part of a single CPU chip <span>[4]</span>. Increment in the clock frequency of a single CPU core (silicon based) is virtually no more feasible due to the physical and practical obstacles. Multicore technology is the posed and accepted solution to this limitation.</p><p id="B9780124080898000033-p0050">Another sophisticated architectural innovation in several modern CPU architectures is the multithreading facility per CPU core. A physical core acts as to provide more than one (usually two) <em>logical processors</em> that might be benefited by the application in hand. The common realizations of this concept include <em>hyperthreading</em>, <em>symmetric multithreading</em> (SMT), and <em>chip multithreading</em> (CMT). A concise introduction to this topic and to the overall features of modern processors is given by Hager and Wellein (<span>[5]</span>, 1–36). Implications of several of the architectural features of the modern processors (especially multicore, multithreading, and ILP) are discussed in the coming sections.</p></section></div><p class="u-padding-s-top u-padding-xs-bottom read-full-chapter-link"><a href="/science/article/pii/B9780124080898000033" class="button-alternative button-alternative-primary" aria-describedby="tp-snippet-chp-title-B9780124080898000033" data-aa-region="aa-tp-snippet-read-full-chp" data-aa-name="" data-hack="#"><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-right button-alternative-icon"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg><span class="button-alternative-text">Read full chapter</span></a></p></div><div class="chapter-pdf-download u-show-from-md"><div class="hor-line" style="border-color:#EBEBEB"></div><button class="button-link u-padding-s-ver button-link-primary text-s" data-aa-region="aa-tp-purchase-book" data-aa-name="" type="button"><svg focusable="false" viewBox="0 0 98 128" width="18.375" height="24" class="icon icon-download"><path d="m77.38 56.18l-6.6-7.06-16.78 17.24v-40.36h-1e1v40.34l-17.72-17.24-7.3 7.08 29.2 29.32 29.2-29.32m10.62 17.82v2e1h-78v-2e1h-1e1v3e1h98v-3e1h-1e1"></path></svg><span class="button-link-text">Purchase book</span></button><div class="u-padding-m-bottom"></div></div></article><article class="snippet"><div class="snippet-inner"><h2 class="u-font-serif u-text-light" id="tp-snippet-chp-title-B9780750689762000031"><a class="anchor" href="/science/article/pii/B9780750689762000031" data-aa-region="aa-tp-snippet-chp-title" data-aa-name="DSP System General Model" data-hack="#"><span class="anchor-text">DSP System General Model</span></a></h2><div class="size-l"><cite><p><span>James D. Broesch, in </span><a href="/book/9780750689762" class="anchor" data-aa-region="aa-tp-snippet-bk-title" data-aa-name="Digital Signal Processing" data-hack="#"><span class="anchor-text">Digital Signal Processing</span></a>, 2009</p></cite><section id="B9780750689762000031-s0110"><h3>Program Store, Data Store</h3><div><p>The <em>program store</em><span> stores the instructions used in implementing the required DSP algorithms. In a general-purpose computer (<span class="topic-highlight">von Neumann architecture</span>), data and instructions are stored together. In most DSP systems, the program is stored separately from the data, since this allows faster execution of the instructions. Data can be moved on its own bus at the same time that instructions are being fetched. This architecture was developed from basic research performed at Harvard University, and therefore is generally called a </span><em>Harvard architecture.</em> Often the data bus and the instruction bus have different widths.</p><div class="panel-s"><h3>Insider Info</h3><p id="B9780750689762000031-p0280"><em>Quite often, three system buses can be found on DSP systems, one for instructions, one for data (including I/O) and one for transferring coefficients from a separate memory area or chip</em>.</p></div></div></section></div><p class="u-padding-s-top u-padding-xs-bottom read-full-chapter-link"><a href="/science/article/pii/B9780750689762000031" class="button-alternative button-alternative-primary" aria-describedby="tp-snippet-chp-title-B9780750689762000031" data-aa-region="aa-tp-snippet-read-full-chp" data-aa-name="" data-hack="#"><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-right button-alternative-icon"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg><span class="button-alternative-text">Read full chapter</span></a></p></div><div class="chapter-pdf-download u-show-from-md"><div class="hor-line" style="border-color:#EBEBEB"></div><button class="button-link u-padding-s-ver button-link-primary text-s" data-aa-region="aa-tp-purchase-book" data-aa-name="" type="button"><svg focusable="false" viewBox="0 0 98 128" width="18.375" height="24" class="icon icon-download"><path d="m77.38 56.18l-6.6-7.06-16.78 17.24v-40.36h-1e1v40.34l-17.72-17.24-7.3 7.08 29.2 29.32 29.2-29.32m10.62 17.82v2e1h-78v-2e1h-1e1v3e1h98v-3e1h-1e1"></path></svg><span class="button-link-text">Purchase book</span></button><div class="u-padding-m-bottom"></div></div></article></div></div></section></main><div id="footer" role="contentinfo"><div class="hor-line-top u-padding-s-top u-bg-white u-clr-grey7" style="border-color:#e9711c"></div><a class="anchor u-padding-s-left move-left els-footer-elsevier" href="https://www.elsevier.com/" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text"><svg viewBox="-3345 3440.027 140.01 24.333" style="width:104px;height:30px"><title>Elsevier</title><path id="E" style="fill:#E9711C" d="M-3343.999,3461.698c2.24,0,3.026-0.473,3.026-2.892v-13.393c0-2.42-0.785-2.89-3.026-2.891v-0.59 h16.787l0.252,4.455h-0.56c-0.308-2.48-1.513-3.216-3.84-3.216h-5.325c-1.26,0-1.26,0.355-1.26,2.066v5.693h5.913 c1.934,0,2.522-0.826,2.718-2.803h0.56v6.844h-0.56c-0.168-1.946-0.813-2.802-2.718-2.802h-5.913v6.401 c0,1.858,0.11,2.476,1.4,2.476h5.914c2.522,0,4.092-1.62,4.82-3.952l0.532,0.207l-1.513,4.985H-3344L-3343.999,3461.698"></path><path style="fill:#E9711C" d="M-3325.448,3461.698c2.074-0.118,2.83-0.502,2.83-2.832v-13.511c0-2.33-0.757-2.715-2.83-2.832v-0.591h8.884 v0.591c-2.243,0-3.027,0.472-3.027,2.891v13.009c0,1.652,0.056,2.625,1.71,2.625h4.008c3,0,4.4-2,5.576-4.1l0.673,0.118 l-1.71,5.222h-16.114V3461.698"></path><path style="fill:#E9711C" d="M-3307.122,3456.27h0.561c1.12,2.596,2.886,5.4,5.94,5.4c2,0,3.672-1.3,3.672-3.334 c0-1.652-1.626-3.1-4.176-4.927c-3.28-2.36-5.41-3.746-5.41-6.43c0-3.422,2.55-5.517,5.633-5.517c2.214,0,3,0.944,4.204,0.944 c0.476,0,0.56-0.266,0.476-0.737h0.561l0.645,6.076h-0.561c-0.785-2.625-2.523-5.19-5.354-5.19c-1.737,0-3.138,1.356-3.138,3.185 c0,1.918,1.933,3.157,5.016,5.016c2.523,1.504,5,3.362,5,6.254c0,3.245-2.608,5.752-5.97,5.752c-2.02,0-4.148-0.855-4.68-0.855 c-0.336,0-0.7,0.207-0.756,0.649h-0.56l-1.094-6.282"></path><path style="fill:#E9711C" d="M-3293.999,3461.698c2.24,0,3.026-0.473,3.026-2.892v-13.393c0-2.42-0.785-2.89-3.026-2.891v-0.59 h16.787l0.252,4.455h-0.56c-0.308-2.48-1.513-3.216-3.84-3.216h-5.325c-1.26,0-1.26,0.355-1.26,2.066v5.693h5.913 c1.934,0,2.522-0.826,2.718-2.803h0.56v6.844h-0.56c-0.168-1.946-0.813-2.802-2.718-2.802h-5.913v6.401 c0,1.858,0.11,2.476,1.4,2.476h5.914c2.522,0,4.092-1.62,4.82-3.952l0.532,0.207l-1.513,4.985H-3294L-3293.999,3461.698"></path><path style="fill:#E9711C" d="M-3265.839,3462.524h-0.42l-5.41-12.538c-0.896-2.065-1.71-4.16-2.83-6.136 c-0.478-0.826-1.346-1.327-2.3-1.327v-0.591h8.323v0.591c-0.785,0-2.354,0-2.354,1.15c0,0.384,0.87,2.45,1.653,4.308l4.063,9.676 l4.877-11.359c0.588-1.356,0.757-2.094,0.757-2.713c0-0.618-0.673-0.974-2.13-1.062v-0.59h5.941v0.591 c-0.337,0.06-0.7,0.117-1.037,0.295c-1.066,0.56-2.13,3.57-2.635,4.749l-6.5,14.957"></path><path style="fill:#E9711C" d="M-3255.472,3461.698c2.24,0,3.025-0.473,3.025-2.892v-13.393c0-2.42-0.784-2.89-3.025-2.891v-0.59h9.078v0.591 c-2.24,0-3.025,0.472-3.025,2.891v13.393c0,2.42,0.784,2.892,3.025,2.892v0.59h-9.08v-0.59"></path><path id="E_2_" style="fill:#E9711C" d="M-3244.999,3461.698c2.24,0,3.026-0.473,3.026-2.892v-13.393c0-2.42-0.785-2.89-3.026-2.891v-0.59 h16.787l0.252,4.455h-0.56c-0.308-2.48-1.513-3.216-3.84-3.216h-5.325c-1.26,0-1.26,0.355-1.26,2.066v5.693h5.913 c1.934,0,2.522-0.826,2.718-2.803h0.56v6.844h-0.56c-0.168-1.946-0.813-2.802-2.718-2.802h-5.913v6.401 c0,1.858,0.11,2.476,1.4,2.476h5.914c2.522,0,4.092-1.62,4.82-3.952l0.532,0.207l-1.513,4.985H-3245L-3244.999,3461.698"></path><path style="fill:#E9711C" d="M-3206,3461.698c-1.26-0.354-1.71-0.68-2.466-1.623l-6.166-7.609c3.027-0.65,5.13-2.185,5.13-5.547 c0-4.75-4.26-4.986-7.764-4.986h-9.191v0.591c2.24,0,3.026,0.472,3.026,2.891v13.393c0,2.42-0.785,2.892-3.026,2.892v0.59h9.08 v-0.59c-2.242,0-3.027-0.473-3.027-2.892v-5.604h2.551l7.314,9.086h4.54L-3206,3461.698 M-3220.399,3444.499 c0-1.387,0.337-1.476,2.186-1.476c2.774,0,5.3,0.974,5.3,4.308c0,3.6-2.887,4.63-5.914,4.631h-1.569v-7.463H-3220.399z"></path></svg></span></a><div class="panel-s u-bg-white u-padding-s-hor-from-md u-padding-xs-ver text-xs u-clear-both-from-xs u-clear-none-from-md"><ul class="u-margin-xs-bottom" style="list-style:none"><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.elsevier.com/solutions/sciencedirect" id="els-footer-about-science-direct" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">About ScienceDirect</span></a><wbr /></li><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="/customer/authenticate/manra" id="els-footer-remote-access" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">Remote access</span></a><wbr /></li><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://sd-cart.elsevier.com/?" id="els-footer-shopping-cart" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">Shopping cart</span></a></li><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="http://elsmediakits.com" id="els-footer-advertise" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">Advertise</span></a><wbr /></li><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://service.elsevier.com/app/contact/supporthub/sciencedirect/" id="els-footer-contact-support" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">Contact and support</span></a><wbr /></li><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.elsevier.com/legal/elsevier-website-terms-and-conditions" id="els-footer-terms-condition" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">Terms and conditions</span></a><wbr /></li><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.elsevier.com/legal/privacy-policy" id="els-footer-privacy-policy" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">Privacy policy</span></a></li></ul><p id="els-footer-cookie-message">We use cookies to help provide and enhance our service and tailor content and ads. By continuing you agree to the <a class="anchor u-margin-0-right" href="https://www.sciencedirect.com/legal/use-of-cookies" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">use of cookies</span></a>.</p><p id="els-footer-copyright">Copyright © 2019 Elsevier B.V. or its licensors or contributors. ScienceDirect ® is a registered trademark of Elsevier B.V.</p></div><a class="anchor u-padding-l-bottom u-padding-s-hor u-position-relative move-bottom u-float-left-from-xs u-float-right-from-md move-right u-padding-0-hor u-margin-top-xs els-footer-relx" href="https://www.relx.com/" aria-label="RELX home page (opens in a new tab)" id="els-footer-relx-group" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text"><svg xmlns="http://www.w3.org/2000/svg" width="78" height="30" version="1" viewBox="0 0 280 65" alt="RELX group home page"><g transform="matrix(.13333 0 0 -.13333 0 65)"><path fill="#f4741e" d="M207 251c110 0 242 26 242 131 0 75-77 103-146 103C196 485 0 425 0 215 0 110 80 49 208 49c135 0 232 87 262 205C391 118 291 75 209 75 107 75 68 151 68 216c0 185 144 248 235 248 61 0 97-40 97-83 0-119-172-116-250-116h-38l-16-17c36-5 85-15 139-42C347 149 453 0 592 0c51 0 64 15 74 29-69-44-183 32-245 90-51 48-96 103-214 132"></path><path fill="#666666" d="M886 310c0 33-22 53-55 53h-77c-2 0-3-1-3-3v-99c0-2 1-3 3-3h77c33 0 55 20 55 52m7-261c-5 0-7 2-8 6l-69 145h-62c-2 0-3-1-3-4V55c0-4-2-6-6-6h-51c-4 0-6 2-6 6v361c0 3 2 5 6 5h139c66 0 115-45 115-111 0-49-27-86-69-102l76-152c2-4 0-7-4-7h-58M1010 416c0 3 3 5 6 5h218c4 0 6-2 6-5v-47c0-3-2-6-6-6h-158c-2 0-3-1-3-3v-91c0-2 1-3 3-3h127c4 0 6-2 6-6v-47c0-3-2-5-6-5h-127c-2 0-3-1-3-3v-95c0-2 1-3 3-3h158c4 0 6-2 6-5V55c0-4-2-6-6-6h-218c-3 0-6 2-6 6v361M1298 416c0 3 2 5 5 5h52c3 0 6-2 6-5V110c0-2 1-3 3-3h147c3 0 5-2 5-5V55c0-4-2-6-5-6h-208c-3 0-5 2-5 6v361M1775 49c-4 0-6 1-8 5l-73 127h-1l-73-127c-2-4-4-5-8-5h-58c-3 0-5 3-3 7l108 185-100 173c-2 4 0 7 3 7h58c4 0 6-2 8-6l65-112h1l65 112c2 4 5 6 9 6h57c4 0 5-3 3-7l-99-173 107-185c2-4 1-7-3-7h-58M1912 295l-2 1v104l-1 1h-32l-2 2v16l2 2h87l2-2v-16l-2-2h-32l-1-1V296l-2-1h-17M1988 419l2 2h16c1 0 2 0 3-2l33-78h1l33 78c1 2 2 2 3 2h16l2-2V296l-2-1h-16l-1 1v78h-1l-26-60-3-2h-11l-3 2-26 60h-1v-78l-2-1h-15l-2 1v123"></path></g></svg></span></a></div></div><script id="store-json" type="b921dd7e5908c3e856e29bb6-text/javascript">
                          var reduxData = {"config":{"useHtml2Pdf":false,"topicSlug":"von-neumann-architecture","domainSlug":"computer-science","topicIndexPageUrl":"","cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","assetsBaseUrl":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/78869a99bb515d27b0258c9bf222cd4d21e6a37f","cachebusterQueryParam":"?_","baseEndpoint":"","authData":{"session":{"ssoKey":"af455687430a16466b5a606954f782847030gxrqb|$|3E157C436B20267B47B2467B1B747AA7CECA729EFDB83AEAF3D1937F0F1FF8E70DED86A8EF726A814334A0DEE25027A8C011124731FD6B9A0E9169905BBD791CB0469A67597464825D387A21AFA2E514","sdSessionId":"af455687430a16466b5a606954f782847030gxrqb","usageInfo":"(12975512,U|291352,D|228598,A|3,P|2,PL)(SDFE,CON|af455687430a16466b5a606954f782847030gxrqb,SSO|ANON_GUEST,ACCESS_TYPE)","extraInfo":{"accessType":"GUEST","csasAssocType":"GUEST","customer":false,"userAnonymity":"ANON_GUEST"},"miamiSessionId":"1a363fca-e9a5-476a-9b4d-fe9cd8cca1ef"},"hasMultipleOrganizations":false,"organization":{"department":"ScienceDirect Guests","departmentId":"291352","account":"ScienceDirect Guests","accountNumber":"C000228598","accountId":"228598"},"user":{"webUserId":"12975512","type":"NORMAL","upc":"C99","settings":{"canActivatePersonalization":true,"canViewPurchaseHistory":true,"canAccessRemotely":false,"forceAbstractView":false,"blockFreeAbstracts":false,"canBypassConfirmScreen":true,"requireCostCode":false,"creditCardPurchaseAllowed":true,"preventTransactionalAccess":false,"preventDocumentDelivery":true,"blockFullTextForAnonymousAccess":false,"crawlerAvailable":false,"allowAnonTransaction":false,"disableWholeIssueDownload":false,"allowRestrictedHtml":false,"blockGateway":false}},"accessType":"GUEST","idAbValue":"C:0:3","cookies":["MIAMISESSION=1a363fca-e9a5-476a-9b4d-fe9cd8cca1ef:3751671817;Version=1;Domain=.sciencedirect.com;Path=/;HttpOnly"],"isFallbackUser":false},"environment":"prod","enableGlobalHeader":true,"enableGlobalHeaderSearch":true,"cdnBaseUrl":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","supportHubId":25793,"requestPath":"/topics/computer-science/von-neumann-architecture","arsCDN":"https://ars.els-cdn.com","enableAnnotations":true,"enableAnnotationsForIE11Users":true,"enableEntitlementChecks":false,"enableMathJax":true,"enablePdfDownload":true,"searchAlerts":{"enableDailyAlerts":false}},"topic":{"name":"Von Neumann Architecture","definition":null,"snippets":[{"body":{"#name":"section","$":{"view":"all","id":"s0325","xmlns:ce":""},"$$":[{"#name":"label","_":"3.1"},{"#name":"section-title","$":{"id":"st0335"},"_":"Processor Architectures and Security Flaws"},{"#name":"para","$":{"view":"all","id":"p1330"},"_":"The Von Neumann architecture, also known as the Princeton architecture, is a computer architecture based on that described in 1945 by the mathematician and physicist John Von Neumann. He described an architecture for an electronic digital computer with parts consisting of a processing unit containing an arithmetic logic unit (ALU) and processor registers, a control unit containing an instruction register and program counter (PC), a memory to store both data and instructions, external mass storage, and input and output mechanisms. The meaning has evolved to be any stored-program computer in which an instruction fetch and a data operation cannot occur at the same time because they share a common bus."},{"#name":"para","$":{"view":"all","id":"p1335"},"_":"The design of a Von Neumann architecture is simpler than the more modern Harvard architecture which is also a stored-program system but has one dedicated set of address and data buses for reading data from and writing data to memory, and another set of address and data buses for fetching instructions. A stored-program digital computer is one that keeps its program instructions, as well as its data, in read-write, random-access memory."},{"#name":"para","$":{"view":"all","id":"p1340"},"_":"A stored-program design also allows for self-modifying code. One early motivation for such a facility was the need for a program to increment or otherwise modify the address portion of instructions, which had to be done manually in early designs. This became less important when index registers and indirect addressing became usual features of machine architecture. Another use was to embed frequently used data in the instruction stream using immediate addressing. Self-modifying code has largely fallen out of favor, since it is usually hard to understand and debug, as well as being inefficient under modern processor pipelining and caching schemes."},{"#name":"para","$":{"view":"all","id":"p1345"},"_":"On a large scale, the ability to treat instructions as data is what makes assemblers, compilers, linkers, loaders, and other automated programming tools possible. One can “write programs which write programs.” On a smaller scale, repetitive I/O-intensive operations such as the BITBLT image manipulation primitive or pixel &amp; vertex shaders in modern 3D graphics were considered inefficient to run without custom hardware. These operations could be accelerated on general purpose processors with “on the fly compilation” (“just-in-time compilation”) technology, e.g., code-generating programs—one form of self-modifying code that has remained popular."},{"#name":"para","$":{"view":"all","id":"p1350"},"_":"There are drawbacks to the Von Neumann design especially when it comes to security, which was not even conceived as a problem until the 1980s. Program modifications can be quite harmful, either by accident or design. Since the processor just executes the word the PC points to, there is effectively no distinction between instructions and data. This is precisely the design flaw that attackers use to perform code injection attacks and it leads to the theme of the inherently secure processor: the processor cooperates in security."}]},"pii":"B9780128024591000063","isbn":"9780128024591","issn":null,"version":"S300.1","contentType":"BK","cid":"315885","title":"Security in embedded systems<ce:cross-ref id=\"cf0515\" refid=\"fn8000\">*</ce:cross-ref><ce:footnote id=\"fn8000\"><ce:label>*</ce:label><ce:note-para view=\"all\" id=\"np8000\">Please visit the companion website <ce:inter-ref id=\"ir9000\" xlink:href=\"http://booksite.elsevier.com/9780128024591\" xlink:type=\"simple\">http://booksite.elsevier.com/9780128024591</ce:inter-ref> for part two of this chapter which covers the following topics in detail: Important Security Concepts, Security And Network Architecture, Software Vulnerability And Cyber Attacks, Security And Operating System Architecture.</ce:note-para></ce:footnote>","displayName":"Rugged Embedded Systems","coverDateYear":"2017","authors":{"#name":"rootwrapper","$":{"xmlns:ce":""},"$$":[{"#name":"author-group","$":{"id":"ag0005"},"$$":[{"#name":"author","$":{"id":"au0010","author-id":"B9780128024591000063-ef5dcd6edece9f406bef136873153f0b"},"$$":[{"#name":"given-name","_":"J."},{"#name":"surname","_":"Rosenberg"}]},{"#name":"affiliation","$":{"id":"af0010"},"$$":[{"#name":"textfn","$":{"id":"tn0010"},"_":"Draper Laboratory, Cambridge, MA, United States"},{"#name":"affiliation","$":{"xmlns:sa":"http://www.elsevier.com/xml/common/struct-aff/dtd"},"$$":[{"#name":"organization","_":"Draper Laboratory"},{"#name":"city","_":"Cambridge"},{"#name":"state","_":"MA"},{"#name":"country","_":"United States"}]}]}]}]},"floats":[],"attachments":[],"snippetMetadata":{"identifier":"B9780128024591000063","xpath":"//ce:section[@id='s0325']","version":"S300.1"}},{"body":{"#name":"section","$":{"view":"all","id":"s0120","xmlns:ce":""},"$$":[{"#name":"label","_":"14.8"},{"#name":"section-title","$":{"id":"st0135"},"_":"Summary"},{"#name":"para","$":{"view":"all","id":"p1730"},"$$":[{"#name":"list","$":{"id":"l0120"},"$$":[{"#name":"list-item","$":{"id":"o0245"},"$$":[{"#name":"label","_":"1."},{"#name":"para","$":{"view":"all","id":"p1735"},"_":"The Von Neumann architecture consists of a single, shared memory for programs and data, a single bus for memory access, an arithmetic unit, and a program control unit. The Von Neumann processor operates fetching and execution cycles seriously."}]},{"#name":"list-item","$":{"id":"o0250"},"$$":[{"#name":"label","_":"2."},{"#name":"para","$":{"view":"all","id":"p1740"},"_":"The Harvard architecture has two separate memory spaces dedicated to program code and to data, respectively, two corresponding address buses, and two data buses for accessing two memory spaces. The Harvard processor offers fetching and executions in parallel."}]},{"#name":"list-item","$":{"id":"o0255"},"$$":[{"#name":"label","_":"3."},{"#name":"para","$":{"view":"all","id":"p1745"},"_":"The DSP special hardware units include an MAC dedicated to DSP filtering operations, a shifter unit for scaling and address generators for circular buffering."}]},{"#name":"list-item","$":{"id":"o0260"},"$$":[{"#name":"label","_":"4."},{"#name":"para","$":{"view":"all","id":"p1750"},"_":"The fixed-point DSP uses integer arithmetic. The data format Q-15 for the fixed-point system is preferred to avoid the overflows."}]},{"#name":"list-item","$":{"id":"o0265"},"$$":[{"#name":"label","_":"5."},{"#name":"para","$":{"view":"all","id":"p1755"},"_":"The floating-point processor uses the floating-point arithmetic. The standard floating-point formats include the IEEE single-precision and double-precision formats."}]},{"#name":"list-item","$":{"id":"o0270"},"$$":[{"#name":"label","_":"6."},{"#name":"para","$":{"view":"all","id":"p1760"},"_":"The architectures and features of fixed-point processors and floating-point processors were briefly reviewed."}]},{"#name":"list-item","$":{"id":"o0275"},"$$":[{"#name":"label","_":"7."},{"#name":"para","$":{"view":"all","id":"p1765"},"_":"Implementing digital filters in the fixed-point DSP system requires scaling filter coefficients so that the filters are in Q-15 format, and input scaling for adder so that overflow during the MAC operations can be avoided."}]},{"#name":"list-item","$":{"id":"o0280"},"$$":[{"#name":"label","_":"8."},{"#name":"para","$":{"view":"all","id":"p1770"},"_":"The floating-point processor is easy to code using the floating-point arithmetic and develop the prototype quickly. However, it is not efficient in terms of the number of instructions it has to complete compared with the fixed-point processor."}]},{"#name":"list-item","$":{"id":"o0285"},"$$":[{"#name":"label","_":"9."},{"#name":"para","$":{"view":"all","id":"p1775"},"_":"The fixed-point processor using fixed-point arithmetic takes much effort to code. But it offers the least number of the instructions for the CPU to execute."}]},{"#name":"list-item","$":{"id":"o9000"},"$$":[{"#name":"label","_":"10."},{"#name":"para","$":{"view":"all","id":"p9000"},"_":"Additional real-time DSP examples are provided, including adaptive filtering, signal quantization and coding, and sample rate conversion."}]}]}]}]},"pii":"B9780128150719000142","isbn":"9780128150719","issn":null,"version":"S300.2","contentType":"BK","cid":"318910","title":"Hardware and Software for Digital Signal Processors","displayName":"Digital Signal Processing (Third Edition)","coverDateYear":"2019","authors":{"#name":"rootwrapper","$":{"xmlns:ce":""},"$$":[{"#name":"author-group","$":{"xfab-added":"true"},"$$":[{"#name":"author","$":{"id":"au0015","author-id":"C20170023194-39e2b4c8edd702692bba6d90e6fa0a51"},"$$":[{"#name":"given-name","_":"Lizhe"},{"#name":"surname","_":"Tan"}]}]},{"#name":"author-group","$":{"xfab-added":"true"},"$$":[{"#name":"author","$":{"id":"au0010","author-id":"C20170023194-d9405e69c226ca5e1f2c854acb480072"},"$$":[{"#name":"given-name","_":"Jean"},{"#name":"surname","_":"Jiang"}]}]}]},"floats":[],"attachments":[],"snippetMetadata":{"identifier":"B9780128150719000142","xpath":"//ce:section[@id='s0120']","version":"S300.2"}},{"body":{"#name":"section","$":{"view":"all","id":"s0015","xmlns:ce":""},"$$":[{"#name":"section-title","$":{"id":"st0030"},"_":"von Neumann Bottleneck"},{"#name":"para","$":{"view":"all","id":"p0095"},"$$":[{"#name":"__text__","_":"In the classical "},{"#name":"italic","_":"von Neumann architecture"},{"#name":"__text__","_":" a processor (CPU) is connected to main memory through a bus as shown in "},{"#name":"cross-ref","$":{"refid":"f0010","id":"cf0095"},"_":"Fig. 3.1"},{"#name":"anchor","$":{"id":"page48"}},{"#name":"float-anchor","$":{"refid":"f0010"}},{"#name":"__text__","_":". In early computer systems timings for accessing main memory and for computation were reasonably well balanced. However, during the past few decades computation speed grew at a much faster rate compared to main memory access speed resulting in a significant performance gap. This discrepancy between CPU compute speed and main memory (DRAM) speed is commonly known as the "},{"#name":"bold","_":"von Neumann bottleneck"},{"#name":"__text__","_":"."}]},{"#name":"para","$":{"view":"all","id":"p0100"},"$$":[{"#name":"__text__","_":"Let us demonstrate that with an example. Consider a CPU with eight cores and a clock frequency of 3 GHz. Moreover, assume that 16 double-precision "},{"#name":"italic","_":"floating-point operations"},{"#name":"__text__","_":" (Flop) can be performed per core in each clock cycle. This results in a peak compute performance of "},{"#name":"math","$":{"overflow":"scroll","altimg":"si1.gif","xmlns:mml":""},"$$":[{"#name":"mn","_":"3"},{"#name":"mtext","_":"undefined"},{"#name":"mtext","_":"GHz"},{"#name":"mo","_":"×"},{"#name":"mn","_":"8"},{"#name":"mo","_":"×"},{"#name":"mn","_":"16"},{"#name":"mo","_":"="},{"#name":"mn","_":"384"},{"#name":"mtext","_":"undefined"},{"#name":"mtext","_":"GFlop/s"}]},{"#name":"__text__","_":". The CPU is connected to a DRAM module with a peak memory transfer rate of 51.2 GB/s. We want to establish an upper bound on the performance of this system for calculating the dot product of two vectors "},{"#name":"italic","_":"u"},{"#name":"__text__","_":" and "},{"#name":"italic","_":"v"},{"#name":"__text__","_":" containing "},{"#name":"italic","_":"n"},{"#name":"__text__","_":" double-precision numbers stored in main memory:"},{"#name":"list","$":{"id":"ls0025"},"$$":[{"#name":"list-item","$":{"id":"u0010"},"$$":[{"#name":"para","$":{"view":"all","id":"p0105"},"$$":[{"#name":"monospace","$$":[{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"double"},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"dotp"},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"="},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"0.0;"}]},{"#name":"vsp","$":{"sp":"1.0"}},{"#name":"monospace","$$":[{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"for"},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"(int"},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"i"},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"="},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"0;"},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"i"},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"&lt;"},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"n;"},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"i++)"}]},{"#name":"vsp","$":{"sp":"1.0"}},{"#name":"monospace","$$":[{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"dotp"},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"+="},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"u[i]"},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"*"},{"#name":"hsp","$":{"sp":"0.25"}},{"#name":"__text__","_":"v[i];"}]}]}]}]},{"#name":"__text__","_":" We further assume that the length of both vectors "},{"#name":"math","$":{"overflow":"scroll","altimg":"si2.gif"},"$$":[{"#name":"mi","_":"n"},{"#name":"mo","_":"="},{"#name":"msup","$$":[{"#name":"mrow","$$":[{"#name":"mn","_":"2"}]},{"#name":"mrow","$$":[{"#name":"mn","_":"30"}]}]}]},{"#name":"__text__","_":" is sufficiently large. Since two operations (one multiplication and one addition) are performed within each iteration of the loop, we have a total of "},{"#name":"math","$":{"overflow":"scroll","altimg":"si3.gif"},"$$":[{"#name":"mn","_":"2"},{"#name":"mo","_":"⋅"},{"#name":"mi","_":"n"},{"#name":"mo","_":"="},{"#name":"msup","$$":[{"#name":"mrow","$$":[{"#name":"mn","_":"2"}]},{"#name":"mrow","$$":[{"#name":"mn","_":"31"}]}]}]},{"#name":"__text__","_":" Flops to compute. Furthermore, the two vectors have to be transferred from main memory to the CPU. Thus, a total volume of "},{"#name":"math","$":{"overflow":"scroll","altimg":"si4.gif"},"$$":[{"#name":"msup","$$":[{"#name":"mrow","$$":[{"#name":"mn","_":"2"}]},{"#name":"mrow","$$":[{"#name":"mn","_":"31"}]}]},{"#name":"mo","_":"×"},{"#name":"mn","_":"8"},{"#name":"mtext","_":" B"},{"#name":"mo","_":"="},{"#name":"mn","_":"16"}]},{"#name":"__text__","_":" GB of data has to be transferred. Based on our system specification, we can determine the minimum amount of time required for computation and data transfer as follows:"},{"#name":"list","$":{"id":"ls0030"},"$$":[{"#name":"list-item","$":{"id":"u0015"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"view":"all","id":"p0110"},"$$":[{"#name":"bold","_":"Computation:"},{"#name":"__text__","_":" "},{"#name":"math","$":{"overflow":"scroll","altimg":"si5.gif"},"$$":[{"#name":"msub","$$":[{"#name":"mrow","$$":[{"#name":"mi","_":"t"}]},{"#name":"mrow","$$":[{"#name":"mtext","_":"comp"}]}]},{"#name":"mo","_":"="},{"#name":"mfrac","$$":[{"#name":"mrow","$$":[{"#name":"mn","_":"2"},{"#name":"mtext","_":" GFlops"}]},{"#name":"mrow","$$":[{"#name":"mn","_":"384"},{"#name":"mtext","_":" GFlop/s"}]}]},{"#name":"mo","_":"="},{"#name":"mn","_":"5.2"},{"#name":"mtext","_":" ms"}]}]}]},{"#name":"list-item","$":{"id":"u0020"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"view":"all","id":"p0115"},"$$":[{"#name":"bold","_":"Data transfer:"},{"#name":"__text__","_":" "},{"#name":"math","$":{"overflow":"scroll","altimg":"si6.gif"},"$$":[{"#name":"msub","$$":[{"#name":"mrow","$$":[{"#name":"mi","_":"t"}]},{"#name":"mrow","$$":[{"#name":"mtext","_":"mem"}]}]},{"#name":"mo","_":"="},{"#name":"mfrac","$$":[{"#name":"mrow","$$":[{"#name":"mn","_":"16"},{"#name":"mtext","_":" GB"}]},{"#name":"mrow","$$":[{"#name":"mn","_":"51.2"},{"#name":"mtext","_":" GB/s"}]}]},{"#name":"mo","_":"="},{"#name":"mn","_":"312.5"},{"#name":"mtext","_":" ms"}]}]}]}]},{"#name":"__text__","_":" If we overlap computation and data transfer, a lower bound on the total execution time can be derived:"},{"#name":"display","$$":[{"#name":"formula","$":{"id":"deq1"},"$$":[{"#name":"label","_":"(3.1)"},{"#name":"math","$":{"overflow":"scroll","altimg":"si7.gif"},"$$":[{"#name":"msub","$$":[{"#name":"mrow","$$":[{"#name":"mi","_":"t"}]},{"#name":"mrow","$$":[{"#name":"mtext","_":"exec"}]}]},{"#name":"mo","_":"≥"},{"#name":"mi","$":{"mathvariant":"normal"},"_":"max"},{"#name":"mo","_":"⁡"},{"#name":"mo","$":{"stretchy":"true","maxsize":"2.4ex","minsize":"2.4ex"},"_":"("},{"#name":"msub","$$":[{"#name":"mrow","$$":[{"#name":"mi","_":"t"}]},{"#name":"mrow","$$":[{"#name":"mtext","_":"comp"}]}]},{"#name":"mo","_":","},{"#name":"msub","$$":[{"#name":"mrow","$$":[{"#name":"mi","_":"t"}]},{"#name":"mrow","$$":[{"#name":"mtext","_":"mem"}]}]},{"#name":"mo","$":{"stretchy":"true","maxsize":"2.4ex","minsize":"2.4ex"},"_":")"},{"#name":"mo","_":"="},{"#name":"mi","$":{"mathvariant":"normal"},"_":"max"},{"#name":"mo","_":"⁡"},{"#name":"mo","$":{"stretchy":"true","maxsize":"2.4ex","minsize":"2.4ex"},"_":"("},{"#name":"mn","_":"5.2"},{"#name":"mtext","_":" ms"},{"#name":"mo","_":","},{"#name":"mn","_":"312.5"},{"#name":"mtext","_":" ms"},{"#name":"mo","$":{"stretchy":"true","maxsize":"2.4ex","minsize":"2.4ex"},"_":")"},{"#name":"mo","_":"="},{"#name":"mn","_":"312.5"},{"#name":"mtext","_":" ms"},{"#name":"mspace","$":{"width":"0.2em"}},{"#name":"mo","_":"."}]}]}]},{"#name":"anchor","$":{"id":"page49"}},{"#name":"__text__","_":" The data transfer time clearly dominates. Note that each matrix element is only used once and there is no data re-usage. Thus, dot product computation is "},{"#name":"italic","_":"memory bound"},{"#name":"__text__","_":". The corresponding upper bound on the achievable performance can be calculated as "},{"#name":"math","$":{"overflow":"scroll","altimg":"si8.gif"},"$$":[{"#name":"mfrac","$$":[{"#name":"mrow","$$":[{"#name":"msup","$$":[{"#name":"mrow","$$":[{"#name":"mn","_":"2"}]},{"#name":"mrow","$$":[{"#name":"mn","_":"31"}]}]},{"#name":"mtext","_":" Flop"}]},{"#name":"mrow","$$":[{"#name":"mn","_":"312.5"},{"#name":"mtext","_":" ms"}]}]},{"#name":"mo","_":"="},{"#name":"mn","_":"6.4"}]},{"#name":"__text__","_":" GFlop/s – meaning that merely less than 2% of the available peak performance can be achieved."}]},{"#name":"para","$":{"view":"all","id":"p0120"},"$$":[{"#name":"__text__","_":"In order to overcome the von Neumann bottleneck, computer architects have introduced a number of extensions to the classical architecture. One of those is the addition of a fast memory between CPU and main memory called "},{"#name":"bold","_":"cache"},{"#name":"__text__","_":". Modern CPUs typically contain a hierarchy of three levels of cache (L1, L2, L3) and current CUDA-enabled GPUs contain two levels. There is the usual trade-off between capacity and speed, e.g. L1-cache is small but fast and the L3-cache is big but slow. In addition caches could be private for a single core or shared between several cores. In the following, we learn more about how caches work and study a number of programs that make effective use of them."}]}]},"pii":"B9780128498903000034","isbn":"9780128498903","issn":null,"version":"S300.1","contentType":"BK","cid":"318123","title":"Modern Architectures","displayName":"Parallel Programming","coverDateYear":"2018","authors":{"#name":"rootwrapper","$":{"xmlns:ce":""},"$$":[{"#name":"author-group","$":{"xfab-added":"true"},"$$":[{"#name":"author","$":{"id":"au0010","author-id":"C2015002113X-e2ade0cdf428a2f3b0b3a07db2cfec3f"},"$$":[{"#name":"given-name","_":"Bertil"},{"#name":"surname","_":"Schmidt"}]},{"#name":"affiliation","$":{"affiliation-id":"C2015002113X-2024582f09ce135896e158f9d84e35ec","id":"aff0010"},"$$":[{"#name":"textfn","_":"Institut für Informatik, Staudingerweg 9, 55128, Mainz, Germany"},{"#name":"affiliation","$":{"xmlns:sa":"http://www.elsevier.com/xml/common/struct-aff/dtd"},"$$":[{"#name":"organization","_":"Institut für Informatik"},{"#name":"address-line","_":"Staudingerweg 9"},{"#name":"city","_":"Mainz"},{"#name":"postal-code","_":"55128"},{"#name":"country","_":"Germany"}]}]}]},{"#name":"author-group","$":{"xfab-added":"true"},"$$":[{"#name":"author","$":{"id":"au0020","author-id":"C2015002113X-58b34fb5b657be918ceefa58d0c84585"},"$$":[{"#name":"given-name","_":"Jorge"},{"#name":"surname","_":"González-Domínguez"}]},{"#name":"affiliation","$":{"affiliation-id":"C2015002113X-f84770cbe5775f303f62fd35c3b53d40","id":"aff0020"},"$$":[{"#name":"textfn","_":"Computer Architecture Group, University of A Coruña, Edificio área científica (Office 3.08), Campus de Elviña, 15071, A Coruña, Spain"},{"#name":"affiliation","$":{"xmlns:sa":"http://www.elsevier.com/xml/common/struct-aff/dtd"},"$$":[{"#name":"organization","_":"Computer Architecture Group"},{"#name":"organization","_":"University of A Coruña"},{"#name":"address-line","_":"Edificio área científica (Office 3.08), Campus de Elviña"},{"#name":"city","_":"A Coruña"},{"#name":"postal-code","_":"15071"},{"#name":"country","_":"Spain"}]}]}]},{"#name":"author-group","$":{"xfab-added":"true"},"$$":[{"#name":"author","$":{"id":"au0030","author-id":"C2015002113X-0d3854a94e2b80fc051dd9a5e2a6a650"},"$$":[{"#name":"given-name","_":"Christian"},{"#name":"surname","_":"Hundt"}]},{"#name":"affiliation","$":{"affiliation-id":"C2015002113X-2024582f09ce135896e158f9d84e35ec","id":"aff0030"},"$$":[{"#name":"textfn","_":"Institut für Informatik, Staudingerweg 9, 55128, Mainz, Germany"},{"#name":"affiliation","$":{"xmlns:sa":"http://www.elsevier.com/xml/common/struct-aff/dtd"},"$$":[{"#name":"organization","_":"Institut für Informatik"},{"#name":"address-line","_":"Staudingerweg 9"},{"#name":"city","_":"Mainz"},{"#name":"postal-code","_":"55128"},{"#name":"country","_":"Germany"}]}]}]},{"#name":"author-group","$":{"xfab-added":"true"},"$$":[{"#name":"author","$":{"id":"au0040","author-id":"C2015002113X-bf2b032a76a614f7b54a3fe22227bc63"},"$$":[{"#name":"given-name","_":"Moritz"},{"#name":"surname","_":"Schlarb"}]},{"#name":"affiliation","$":{"affiliation-id":"C2015002113X-ab24d469885bc6a364033d2520ae5526","id":"aff0040"},"$$":[{"#name":"textfn","_":"Data Center, Johannes Gutenberg-University Mainz, Germany"},{"#name":"affiliation","$":{"xmlns:sa":"http://www.elsevier.com/xml/common/struct-aff/dtd"},"$$":[{"#name":"organization","_":"Data Center"},{"#name":"organization","_":"Johannes Gutenberg-University Mainz"},{"#name":"country","_":"Germany"}]}]},{"#name":"affiliation","$":{"affiliation-id":"C2015002113X-84f18a2d797bbf30709f5ff772ac2408","id":"aff0050"},"$$":[{"#name":"textfn","_":"Anselm-Franz-von-Bentzel-Weg 12, 55128, Mainz, Germany"},{"#name":"affiliation","$":{"xmlns:sa":"http://www.elsevier.com/xml/common/struct-aff/dtd"},"$$":[{"#name":"address-line","_":"Anselm-Franz-von-Bentzel-Weg 12"},{"#name":"city","_":"Mainz"},{"#name":"postal-code","_":"55128"},{"#name":"country","_":"Germany"}]}]}]}]},"floats":[{"#name":"figure","$":{"id":"f0010"},"$$":[{"#name":"label","_":"Figure 3.1"},{"#name":"caption","$":{"id":"cp0010"},"$$":[{"#name":"simple-para","$":{"view":"all","id":"sp0010"},"_":"Basic structure of a classical von Neumann architecture."}]},{"#name":"alt-text","$":{"role":"short","id":"at0010"},"_":"Figure 3.1"},{"#name":"link","$":{"xmlns:xlink":"","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","href":"pii:B9780128498903000034/gr001","id":"ln0010","type":"simple","locator":"gr001"}}]}],"attachments":[{"attachment-eid":"3-s2.0-B9780128498903000034-gr001.jpg","file-basename":"gr001","filename":"gr001.jpg","pixel-height":"124","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"3-s2.0-B9780128498903000034-gr001.sml","file-basename":"gr001","filename":"gr001.sml","pixel-height":"95","attachment-type":"IMAGE-THUMBNAIL"}],"snippetMetadata":{"identifier":"B9780128498903000034","xpath":"//ce:section[@id='s0015']","version":"S300.1"}},{"body":{"#name":"section","$":{"view":"all","id":"s0015","xmlns:ce":""},"$$":[{"#name":"label","_":"2.1.1"},{"#name":"section-title","$":{"id":"st0015"},"_":"The von Neumann architecture"},{"#name":"para","$":{"view":"all","id":"p0025"},"$$":[{"#name":"__text__","_":"The “classical” "},{"#name":"bold","_":"von Neumann architecture"},{"#name":"__text__","_":" consists of "},{"#name":"bold","_":"main memory"},{"#name":"__text__","_":", a "},{"#name":"bold","_":"central-processing unit"},{"#name":"__text__","_":" (CPU) or "},{"#name":"bold","_":"processor"},{"#name":"__text__","_":" or "},{"#name":"bold","_":"core"},{"#name":"__text__","_":", and an "},{"#name":"bold","_":"interconnection"},{"#name":"__text__","_":" between the memory and the CPU. Main memory consists of a collection of locations, each of which is capable of storing both instructions and data. Every location consists of an address, which is used to access the location and the contents of the location—the instructions or data stored in the location."}]},{"#name":"para","$":{"view":"all","id":"p0030"},"$$":[{"#name":"anchor","$":{"id":"p16"}},{"#name":"__text__","_":"The central processing unit is divided into a control unit and an arithmetic and logic unit (ALU). The control unit is responsible for deciding which instructions in a program should be executed, and the ALU is responsible for executing the actual instructions. Data in the CPU and information about the state of an executing program are stored in special, very fast storage called "},{"#name":"bold","_":"registers"},{"#name":"__text__","_":". The control unit has a special register called the "},{"#name":"bold","_":"program counter"},{"#name":"__text__","_":". It stores the address of the next instruction to be executed."}]},{"#name":"para","$":{"view":"all","id":"p0035"},"$$":[{"#name":"__text__","_":"Instructions and data are transferred between the CPU and memory via the interconnect. This has traditionally been a "},{"#name":"bold","_":"bus"},{"#name":"__text__","_":", which consists of a collection of parallel wires and some hardware controlling access to the wires. A von Neumann machine executes a single instruction at a time, and each instruction operates on only a few pieces of data. See "},{"#name":"cross-ref","$":{"refid":"f0010"},"_":"Figure 2.1"},{"#name":"__text__","_":"."},{"#name":"float-anchor","$":{"refid":"f0010"}}]},{"#name":"para","$":{"view":"all","id":"p0040"},"$$":[{"#name":"__text__","_":"When data or instructions are transferred from memory to the CPU, we sometimes say the data or instructions are "},{"#name":"bold","_":"fetched"},{"#name":"__text__","_":" or "},{"#name":"bold","_":"read"},{"#name":"__text__","_":" from memory. When data are transferred from the CPU to memory, we sometimes say the data are "},{"#name":"bold","_":"written to memory"},{"#name":"__text__","_":" or "},{"#name":"bold","_":"stored"},{"#name":"__text__","_":". The separation of memory and CPU is often called the "},{"#name":"bold","_":"von Neumann bottleneck"},{"#name":"__text__","_":", since the interconnect determines the rate at which instructions and data can be accessed. The potentially vast quantity of data and instructions needed to run a program is effectively isolated from the CPU. In 2010 CPUs are capable of executing instructions more than one hundred times faster than they can fetch items from main memory."},{"#name":"anchor","$":{"id":"p17"}}]},{"#name":"para","$":{"view":"all","id":"p0045"},"_":"In order to better understand this problem, imagine that a large company has a single factory (the CPU) in one town and a single warehouse (main memory) in another. Further imagine that there is a single two-lane road joining the warehouse and the factory. All the raw materials used in manufacturing the products are stored in the warehouse. Also, all the finished products are stored in the warehouse before being shipped to customers. If the rate at which products can be manufactured is much larger than the rate at which raw materials and finished products can be transported, then it's likely that there will be a huge traffic jam on the road, and the employees and machinery in the factory will either be idle for extended periods or they will have to reduce the rate at which they produce finished products."},{"#name":"para","$":{"view":"all","id":"p0050"},"_":"In order to address the von Neumann bottleneck, and, more generally, improve CPU performance, computer engineers and computer scientists have experimented with many modifications to the basic von Neumann architecture. Before discussing some of these modifications, let's first take a moment to discuss some aspects of the software that are used in both von Neumann systems and more modern systems."}]},"pii":"B9780123742605000026","isbn":"9780123742605","issn":null,"version":"S300.2","contentType":"BK","cid":"283067","title":"Parallel Hardware and Parallel Software","displayName":"An Introduction to Parallel Programming","coverDateYear":"2011","authors":{"#name":"rootwrapper","$":{"xmlns:ce":""},"$$":[{"#name":"author-group","$":{"xfab-added":"true"},"$$":[{"#name":"author","$":{"id":"au0010"},"$$":[{"#name":"given-name","_":"Peter S."},{"#name":"surname","_":"Pacheco"}]},{"#name":"affiliation","$":{"id":"aff0010"},"$$":[{"#name":"textfn","$":{"id":"tf0010"},"_":"University of San Francisco"}]}]}]},"floats":[{"#name":"figure","$":{"id":"f0010"},"$$":[{"#name":"label","_":"Figure 2.1"},{"#name":"caption","$$":[{"#name":"simple-para","$":{"view":"all","id":"sp0010"},"_":"The von Neumann architecture"}]},{"#name":"link","$":{"locator":"f02-01-9780123742605"}}]}],"attachments":[{"attachment-eid":"3-s2.0-B9780123742605000026-f02-01-9780123742605.jpg","file-basename":"f02-01-9780123742605","filename":"f02-01-9780123742605.jpg","pixel-height":"429","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"3-s2.0-B9780123742605000026-f02-01-9780123742605.sml","file-basename":"f02-01-9780123742605","filename":"f02-01-9780123742605.sml","pixel-height":"164","attachment-type":"IMAGE-THUMBNAIL"}],"snippetMetadata":{"identifier":"B9780123742605000026","xpath":"//ce:section[@id='s0015']","version":"S300.2"}},{"body":{"#name":"section","$":{"view":"all","id":"s0110","xmlns:ce":""},"$$":[{"#name":"section-title","$":{"id":"st0080"},"_":"Conventional Microprocessors"},{"#name":"para","$":{"view":"all","id":"p0310"},"$$":[{"#name":"__text__","_":"A conventional microprocessor commonly uses a "},{"#name":"italic","_":"von Neumann"},{"#name":"__text__","_":" architecture, which means that there is only one common system bus used for transfer of both instructions and data between the external memory chips and the processor (see "},{"#name":"cross-ref","$":{"refid":"f0020"},"_":"Figure 8.2"},{"#name":"float-anchor","$":{"refid":"f0020"}},{"#name":"__text__","_":"). The system bus consists of the three sub-buses: the data bus,"},{"#name":"anchor","$":{"id":"p142"}},{"#name":"__text__","_":" the address bus and the control bus. In many cases, the same system bus is also used for I/O operations. In signal processing applications, this single bus is a bottleneck. Execution of the 10-tap FIR filter ("},{"#name":"cross-ref","$":{"refid":"eqn2"},"_":"Equation 8.2"},{"#name":"__text__","_":") will, for instance, require at least 60 bus cycles for instruction fetches and 40 bus cycles for data and coefficient transfers, a total of approximately 100 bus cycles. Hence, even if we are using a fast processor, the speed of the bus cycle will be a limiting factor."}]},{"#name":"para","$":{"view":"all","id":"p0320"},"$$":[{"#name":"__text__","_":"One way to ease this problem is the introduction of "},{"#name":"italic","_":"pipelining"},{"#name":"__text__","_":" techniques, which means that an "},{"#name":"italic","_":"execution unit"},{"#name":"__text__","_":" (EU) and a "},{"#name":"italic","_":"bus unit"},{"#name":"__text__","_":" (BU) on the processor chip work simultaneously. While one instruction is being executed in the EU the next instruction is fetched from memory by the BU and put into an instruction queue, feeding the instruction decoder. In this way, idle bus cycles are eliminated. If a jump instruction occurs in the program, a restart of the instruction queue has however to be performed, causing a delay."}]},{"#name":"para","$":{"view":"all","id":"p0330"},"$$":[{"#name":"__text__","_":"Yet another improvement is to add a "},{"#name":"italic","_":"cache memory"},{"#name":"__text__","_":" on the processor chip. A limited block (some thousand words) of the program code is read into the fast internal cache memory. In this way, instructions can be fetched from the internal cache memory at the same time as data is transferred over the external system bus. This approach may be very efficient in signal processing applications, since in many cases the entire program may fit in the cache, and no reloading is needed."}]},{"#name":"para","$":{"view":"all","id":"p0340"},"$$":[{"#name":"__text__","_":"The execution unit in a conventional microprocessor may consist of an arithmetic logic unit (ALU), a multiplier, a shifter, a floating-point unit (FPU) and some data and flag registers. The ALU commonly handles 2's complement arithmetic, and the FPU uses some standard Institute of Electrical and Electronics Engineers (IEEE) floating-point formats. The "},{"#name":"italic","_":"binary fractions"},{"#name":"__text__","_":" format discussed later in this chapter is often used in signal processing applications but is not supported by general-purpose microprocessors."}]},{"#name":"para","$":{"view":"all","id":"p0350"},"$$":[{"#name":"__text__","_":"Besides program counter (PC) and stack pointer (SP), the "},{"#name":"italic","_":"address unit"},{"#name":"__text__","_":" (AU) of a conventional microprocessor may contain a number of address and "},{"#name":"italic","_":"segment"},{"#name":"__text__","_":" registers. There may also be an ALU for calculating addresses used in complicated addressing modes and/or handling virtual memory functions."}]},{"#name":"para","$":{"view":"all","id":"p0360"},"$$":[{"#name":"__text__","_":"The instruction repertoire of many general-purpose microprocessors supports quite exotic addressing modes which are seldom used in signal processing algorithms. On the other hand, instructions for handling such things like "},{"#name":"italic","_":"delay lines"},{"#name":"__text__","_":" or "},{"#name":"italic","_":"circular buffers"},{"#name":"__text__","_":" in an efficient manner are rare. The MAC operation often requires a number of computer instructions, and loop counters have to be implemented in software, using general-purpose data registers."}]},{"#name":"para","$":{"view":"all","id":"p0370"},"_":"Further, instructions aimed for operating systems and multi-task handling may be found among “higher end” processors. These instructions often are of very limited interest in signal processing applications."},{"#name":"para","$":{"view":"all","id":"p0380"},"$$":[{"#name":"__text__","_":"Most of the common processors today are of the "},{"#name":"italic","_":"complex instruction set computer"},{"#name":"__text__","_":" (CISC) type, i.e. instructions may occupy more than one memory word and hence require more than 1 bus cycle to fetch. Further, these instructions often require more than 1 machine cycle to execute. In many cases,"},{"#name":"anchor","$":{"id":"p143"}},{"#name":"__text__","_":" "},{"#name":"italic","_":"reduced instruction set computer"},{"#name":"__text__","_":" (RlSC)-type processors may perform better in signal-processing applications."}]},{"#name":"section","$":{"view":"all","id":"s0120"},"$$":[{"#name":"section","$":{"view":"all","id":"s0130"},"$$":[{"#name":"section-title","$":{"id":"st0090"},"_":"Technology Trade-offs"},{"#name":"para","$":{"view":"all","id":"p0390"},"_":"In a RISC processor, no instruction occupies more than one memory word; it can be fetched in 1 bus cycle and executes in 1 machine cycle. On the other hand, many RISC instructions may be needed to perform the same function as one CISC-type instruction, but in the RISC case, you can get the required complexity only when needed."},{"#name":"para","$":{"view":"all","id":"p0400"},"_":"Getting analog signals into and out of a general-purpose microprocessor often requires a lot of external hardware. Some microcontrollers have built-in A/D and D/A converters, but in most cases, these converters only have 8- or 12-bit resolution, which is not sufficient in many applications. Sometimes these converters are also quite slow. Even if there are good built-in converters, there is always need for external sample-and-hold (S/H) circuits, and (analog) anti-aliasing and reconstruction filters."},{"#name":"para","$":{"view":"all","id":"p0410"},"$$":[{"#name":"__text__","_":"Some microprocessors have built-in high-speed serial communication circuitry, serial peripheral interface (SPI) or I"},{"#name":"sup","$":{"loc":"post"},"_":"2"},{"#name":"__text__","_":"C™. In such cases we still need to have external converters, but the interface will be easier than using the traditional approach, i.e., to connect the converters in parallel to the system bus. Parallel communication will of course be faster, but the circuits needed will be more complicated and we will be stealing capacity from a common, single system bus."}]},{"#name":"para","$":{"view":"all","id":"p0420"},"$$":[{"#name":"__text__","_":"The interrupt facilities found on many general-purpose processors are in many cases “overkill” for signal processing systems. In this kind of real-time application, timing is crucial and "},{"#name":"italic","_":"synchronous programming"},{"#name":"__text__","_":" is preferred. The number of "},{"#name":"italic","_":"asynchronous events"},{"#name":"__text__","_":", e.g., interrupts, is kept to a minimum. Digital signal processing systems using more than a few interrupt sources are rare. One single interrupt source (be it timing or sample rate) or none is common."}]}]}]}]},"pii":"B9780750689762000080","isbn":"9780750689762","issn":null,"version":"S300.4","contentType":"BK","cid":"277395","title":"Digital Signal Processors","displayName":"Digital Signal Processing","coverDateYear":"2009","authors":{"#name":"rootwrapper","$":{"xmlns:ce":""},"$$":[{"#name":"author-group","$":{"xfab-added":"true"},"$$":[{"#name":"author","$$":[{"#name":"given-name","_":"James D."},{"#name":"surname","_":"Broesch"}]}]}]},"floats":[{"#name":"figure","$":{"id":"f0020"},"$$":[{"#name":"label","_":"Figure 8.2"},{"#name":"caption","$$":[{"#name":"simple-para","$":{"view":"all","id":"sp0020"},"_":"von Neumann architecture, program code and data share memory"}]},{"#name":"link","$":{"id":"lk0020","locator":"gr2"}}]}],"attachments":[{"attachment-eid":"3-s2.0-B9780750689762000080-gr2.jpg","file-basename":"gr2","filename":"gr2.jpg","pixel-height":"164","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"3-s2.0-B9780750689762000080-gr2.sml","file-basename":"gr2","filename":"gr2.sml","pixel-height":"81","attachment-type":"IMAGE-THUMBNAIL"}],"snippetMetadata":{"identifier":"B9780750689762000080","xpath":"//ce:section[@id='s0110']","version":"S300.4"}},{"body":{"#name":"section","$":{"view":"all","id":"s0120","xmlns:ce":""},"$$":[{"#name":"section-title","$":{"id":"cesectitle0135"},"_":"What we learned"},{"#name":"para","$":{"view":"all","id":"p1515"},"$$":[{"#name":"list","$":{"id":"ulist0110"},"$$":[{"#name":"list-item","$":{"id":"u0370"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"view":"all","id":"p1520"},"_":"Both the von Neumann and Harvard architectures are in common use today."}]},{"#name":"list-item","$":{"id":"u0375"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"view":"all","id":"p1525"},"_":"The programming model is a description of the architecture relevant to instruction operation."}]},{"#name":"list-item","$":{"id":"u0380"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"view":"all","id":"p1530"},"_":"ARM is a load-store architecture. It provides a few relatively complex instructions, such as saving and restoring multiple registers."}]},{"#name":"list-item","$":{"id":"u0385"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"view":"all","id":"p1535"},"_":"The PIC16F is a very small, efficient microcontroller."}]},{"#name":"list-item","$":{"id":"u0390"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"view":"all","id":"p1540"},"_":"The C55x provides a number of architectural features to support the arithmetic loops that are common on digital signal processing code."}]},{"#name":"list-item","$":{"id":"u0395"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"view":"all","id":"p1545"},"_":"The C64x organizes instructions into execution packets to enable parallel execution."}]}]}]}]},"pii":"B9780128053874000029","isbn":"9780128053874","issn":null,"version":"S300.3","contentType":"BK","cid":"314916","title":"Instruction Sets","displayName":"Computers as Components (Fourth Edition)","coverDateYear":"2017","authors":{"#name":"rootwrapper","$":{"xmlns:ce":""},"$$":[{"#name":"author-group","$":{"xfab-added":"true"},"$$":[{"#name":"author","$":{"id":"au1"},"$$":[{"#name":"given-name","_":"Marilyn"},{"#name":"surname","_":"Wolf"}]}]}]},"floats":[],"attachments":[],"snippetMetadata":{"identifier":"B9780128053874000029","xpath":"//ce:section[@id='s0120']","version":"S300.3"}},{"body":{"#name":"section","$":{"view":"all","id":"s0020","xmlns:ce":""},"$$":[{"#name":"label","_":"8.3"},{"#name":"section-title","$":{"id":"st0020"},"_":"Model of Computation"},{"#name":"para","$":{"view":"all","id":"p0080"},"$$":[{"#name":"__text__","_":"Recent developments of hardware significantly deviate from the von Neumann architecture; for example, the next generation of processors has multicore processors and several processor cache levels (see "},{"#name":"cross-ref","$":{"refid":"f0010"},"_":"Fig. 8.1"},{"#name":"__text__","_":"). Consequences like "},{"#name":"italic","_":"cache anomalies"},{"#name":"__text__","_":" are well known; for example, recursive programs like "},{"#name":"small-caps","_":"Quicksort"},{"#name":"__text__","_":" perform unexpectedly well in practice when compared to other theoretically stronger sorting algorithms."},{"#name":"float-anchor","$":{"refid":"f0010"}}]},{"#name":"para","$":{"view":"all","id":"p0085"},"$$":[{"#name":"__text__","_":"The commonly used model for comparing the performances of external algorithms consists of a single processor, small internal memory that can hold up to "},{"#name":"italic","_":"M"},{"#name":"__text__","_":" data items, and unlimited secondary memory. The size of the input problem (in terms of the number of records) is abbreviated by "},{"#name":"italic","_":"N"},{"#name":"__text__","_":". Moreover, the "},{"#name":"italic","_":"block size B"},{"#name":"__text__","_":" governs the bandwidth of memory transfers. It is often convenient to refer to these parameters in terms of blocks, so we define "},{"#name":"math","$":{"overflow":"scroll","altimg":"si8.gif","xmlns:mml":""},"$$":[{"#name":"mi","_":"m"},{"#name":"mo","_":"="},{"#name":"mi","_":"M"},{"#name":"mo","_":"∕"},{"#name":"mi","_":"B"}]},{"#name":"__text__","_":" and "},{"#name":"math","$":{"overflow":"scroll","altimg":"si9.gif"},"$$":[{"#name":"mi","_":"n"},{"#name":"mo","_":"="},{"#name":"mi","_":"N"},{"#name":"mo","_":"∕"},{"#name":"mi","_":"B"}]},{"#name":"__text__","_":". It is usually assumed that at the beginning of the algorithm, the input data is stored in contiguous blocks on external memory, and the same must hold for the output. Only the number of block read and writes are counted, and computations in internal memory do not incur any cost (see "},{"#name":"cross-ref","$":{"refid":"f0015"},"_":"Fig. 8.2"},{"#name":"__text__","_":"). An extension of the model considers "},{"#name":"italic","_":"D"},{"#name":"__text__","_":" disks that can be accessed simultaneously. When using disks in parallel, the technique of "},{"#name":"italic","_":"disk striping"},{"#name":"__text__","_":" can be employed to essentially increase the block size by a factor of "},{"#name":"italic","_":"D"},{"#name":"__text__","_":". Successive blocks are "},{"#name":"anchor","$":{"id":"p322"}},{"#name":"anchor","$":{"id":"p323"}},{"#name":"__text__","_":"distributed across different disks. Formally, this means that if we enumerate the records from zero, the "},{"#name":"italic","_":"i"},{"#name":"__text__","_":" th block of the "},{"#name":"italic","_":"j"},{"#name":"__text__","_":" th disk contains record number "},{"#name":"math","$":{"overflow":"scroll","altimg":"si14.gif"},"$$":[{"#name":"mrow","$$":[{"#name":"mo","_":"("},{"#name":"mrow","$$":[{"#name":"mi","_":"i"},{"#name":"mi","_":"D"},{"#name":"mi","_":"B"},{"#name":"mo","_":"+"},{"#name":"mi","_":"j"},{"#name":"mi","_":"B"}]},{"#name":"mo","_":")"}]}]},{"#name":"__text__","_":" through "},{"#name":"math","$":{"overflow":"scroll","altimg":"si15.gif"},"$$":[{"#name":"mrow","$$":[{"#name":"mo","_":"("},{"#name":"mrow","$$":[{"#name":"mi","_":"i"},{"#name":"mi","_":"D"},{"#name":"mi","_":"B"},{"#name":"mo","_":"+"},{"#name":"mrow","$$":[{"#name":"mo","_":"("},{"#name":"mrow","$$":[{"#name":"mspace","$":{"width":"0.1em"}},{"#name":"mi","_":"j"},{"#name":"mo","_":"+"},{"#name":"mn","_":"1"}]},{"#name":"mo","_":")"}]},{"#name":"mi","_":"B"},{"#name":"mo","_":"−"},{"#name":"mn","_":"1"}]},{"#name":"mo","_":")"}]}]},{"#name":"__text__","_":". Usually, it is assumed that "},{"#name":"math","$":{"overflow":"scroll","altimg":"si16.gif"},"$$":[{"#name":"mi","_":"M"},{"#name":"mo","_":"&lt;"},{"#name":"mi","_":"N"}]},{"#name":"__text__","_":" and "},{"#name":"math","$":{"overflow":"scroll","altimg":"si17.gif"},"$$":[{"#name":"mi","_":"D"},{"#name":"mi","_":"B"},{"#name":"mo","_":"&lt;"},{"#name":"mi","_":"M"},{"#name":"mo","_":"∕"},{"#name":"mn","_":"2"}]},{"#name":"__text__","_":"."},{"#name":"float-anchor","$":{"refid":"f0015"}}]},{"#name":"para","$":{"view":"all","id":"p0090"},"$$":[{"#name":"__text__","_":"We distinguish two general approaches of external memory algorithms: either we can devise algorithms to solve specific computational problems while explicitly controlling secondary memory access, or we can develop general-purpose "},{"#name":"italic","_":"external memory data structures"},{"#name":"__text__","_":", such as stacks, queues, search trees, priority queues, and so on, and then use them in algorithms that are similar to their internal memory counterparts."}]}]},"pii":"B9780123725127000080","isbn":"9780123725127","issn":null,"version":"S300.4","contentType":"BK","cid":"280877","title":"External Search","displayName":"Heuristic Search","coverDateYear":"2012","authors":{"#name":"rootwrapper","$":{"xmlns:ce":""},"$$":[{"#name":"author-group","$":{"xfab-added":"true"},"$$":[{"#name":"author","$":{"id":"au0010"},"$$":[{"#name":"given-name","_":"Stefan"},{"#name":"surname","_":"Edelkamp"}]},{"#name":"author","$":{"id":"au0015"},"$$":[{"#name":"given-name","_":"Stefan"},{"#name":"surname","_":"Schrödl"}]}]}]},"floats":[{"#name":"figure","$":{"id":"f0010"},"$$":[{"#name":"label","_":"Figure 8.1"},{"#name":"caption","$$":[{"#name":"simple-para","$":{"view":"all","id":"sp0010"},"_":"The memory hierarchy."}]},{"#name":"link","$":{"locator":"f08-01-9780123725127"}}]},{"#name":"figure","$":{"id":"f0015"},"$$":[{"#name":"label","_":"Figure 8.2"},{"#name":"caption","$$":[{"#name":"simple-para","$":{"view":"all","id":"sp0015"},"_":"The external memory model."}]},{"#name":"link","$":{"locator":"f08-02-9780123725127"}}]}],"attachments":[{"attachment-eid":"3-s2.0-B9780123725127000080-f08-01-9780123725127.jpg","file-basename":"f08-01-9780123725127","filename":"f08-01-9780123725127.jpg","pixel-height":"392","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"3-s2.0-B9780123725127000080-f08-01-9780123725127.sml","file-basename":"f08-01-9780123725127","filename":"f08-01-9780123725127.sml","pixel-height":"164","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"3-s2.0-B9780123725127000080-f08-02-9780123725127.jpg","file-basename":"f08-02-9780123725127","filename":"f08-02-9780123725127.jpg","pixel-height":"242","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"3-s2.0-B9780123725127000080-f08-02-9780123725127.sml","file-basename":"f08-02-9780123725127","filename":"f08-02-9780123725127.sml","pixel-height":"149","attachment-type":"IMAGE-THUMBNAIL"}],"snippetMetadata":{"identifier":"B9780123725127000080","xpath":"//ce:section[@id='s0020']","version":"S300.4"}},{"body":{"#name":"section","$":{"view":"all","id":"s0010","xmlns:ce":""},"$$":[{"#name":"label","_":"2"},{"#name":"section-title","$":{"id":"st010"},"_":"Modern Microprocessor Based Computer Systems"},{"#name":"para","$":{"view":"all","id":"p0025"},"$$":[{"#name":"__text__","_":"The basic physical organization of a modern computer, based on the "},{"#name":"italic","_":"von Neumann architecture"},{"#name":"__text__","_":" model, comprises 5 units, namely memory, control, arithmetic-&amp;-logic, input, and output. The "},{"#name":"italic","_":"central processing unit"},{"#name":"__text__","_":" (CPU) comprises control and arithmetic-&amp;-logic units. The functioning of a computer is precisely the execution of the "},{"#name":"italic","_":"instructions"},{"#name":"__text__","_":" to process the "},{"#name":"italic","_":"data"},{"#name":"__text__","_":" by its CPU. The instructions are the primitive operations that the CPU may execute, such as moving the contents of a memory location (called as "},{"#name":"italic","_":"register"},{"#name":"__text__","_":") to another memory location within the CPU, or adding the contents of two CPU registers. The control unit fetches the data/instruction from the "},{"#name":"italic","_":"system memory"},{"#name":"__text__","_":" or "},{"#name":"italic","_":"main memory"},{"#name":"__text__","_":", sometimes also referred to as the "},{"#name":"italic","_":"random access memory"},{"#name":"__text__","_":" (RAM). The data is then processed by the arithmetic-&amp;-logic unit, "},{"#name":"bold","_":"sequentially"},{"#name":"__text__","_":", according to the instructions decoded by the control unit. Storing both the data and the instructions in a single main memory unit is an essential feature of the von-Neumann architecture. The input and output units provide interface between computer and the human."}]},{"#name":"para","$":{"view":"all","id":"p0030"},"$$":[{"#name":"__text__","_":"Not only the CPU, but also the memory system plays a crucial role in determining the overall computational performance of the computer. The memory system of a modern computer is complicated one. A number of smaller and faster memory units, called "},{"#name":"italic","_":"cache memories"},{"#name":"__text__","_":" or simply "},{"#name":"italic","_":"caches"},{"#name":"__text__","_":", are placed between the CPU and the main memory. The idea of a cache memory is to bring only some part of the program data needed currently from main memory into the cache to speedup the data access by the CPU. The cache memories form a memory hierarchy consisting of a number of levels "},{"#name":"anchor","$":{"id":"p91"}},{"#name":"__text__","_":"in view of their distance from the CPU. The "},{"#name":"italic","_":"access time"},{"#name":"__text__","_":" and "},{"#name":"italic","_":"size"},{"#name":"__text__","_":" of the data increase as the hierarchy level gets away from the CPU. The memory hierarchy (combining smaller and faster caches with larger, slower, and cheaper main memory) behaves most of the time like a fast and large memory. This is mainly due to the fact that the caches are to exploit the feature of "},{"#name":"italic","_":"locality"},{"#name":"__text__","_":" of memory references, also called the "},{"#name":"italic","_":"principle of locality"},{"#name":"__text__","_":", which is often exhibited by the computer programs. Common types of the locality of reference include the "},{"#name":"italic","_":"spatial"},{"#name":"__text__","_":" locality (local in space) and the "},{"#name":"italic","_":"temporal"},{"#name":"__text__","_":" locality (local in time). Spatial locality of reference occurs when a program accesses the data that is stored contiguously (for example, elements of an array) within a short period of time. Caches are used to exploit this feature of spatial locality by pre-fetching from the main memory some data contiguous to the requested one, into a cache. Temporal locality of reference occurs when a program accesses a used data item again after a short period of time (for example, in a loop). Caches are used to exploit this feature of temporal locality by retaining recently used data into a cache for a certain period of time. Note that the locality of reference is a property of the computer programs but is exploited in the memory system design through the caches. This, definitely, indicates that during the coding a programmer should take care to develop the code so as to enhance both the types of localities of reference for efficient cache utilization. This could be achieved by coding in a way that the data is accessed in a sequential/contiguous fashion and, if required to be reused, is accessed again as soon as possible."}]},{"#name":"para","$":{"view":"all","id":"p0035"},"$$":[{"#name":"__text__","_":"A modern CPU (microprocessor) executes (at least) one instruction per clock cycle. Each different type of CPU architecture has its unique set of instructions, called its "},{"#name":"italic","_":"instruction set architecture"},{"#name":"__text__","_":" (ISA). The instruction set architecture of a computer can be thought of the language that the computer can understand. Based on the type of ISA, there are two important classes of modern (microprocessor based) computer architectures: "},{"#name":"italic","_":"CISC"},{"#name":"__text__","_":" (Complex Instruction Set Computer) architecture and "},{"#name":"italic","_":"RISC"},{"#name":"__text__","_":" (Reduced Instruction Set Computer) architecture. The basic CISC architecture is essentially the von Neumann architecture in the sense of storing both the instruction and the data inside a common memory unit. On the other hand, the basic RISC architecture has two entirely separate memory spaces for the instructions and the data, which is the feature that was first introduced in Harvard architecture to overcome the bottleneck in the von Neumann architecture due to data-instruction shared paths between the CPU and the memory. CISC philosophy is that the ISA has a large number of instructions (and addressing modes, as well) with varying number of required clock cycles and execution time. "},{"#name":"anchor","$":{"id":"p92"}},{"#name":"__text__","_":"Also certain instructions can perform multiple primitive operations. RISC philosophy is that the ISA has a small number of primitive instructions for ease in hardware manufacturing and thus the complicated operations are performed, at program level, by combining simpler ones. Due to its very nature, RISC architecture is usually experienced to be faster and efficient than a comparable CISC architecture. However, due to continuing quest for enhancement and flexibility, today a CPU executing an ISA based on CISC may exhibit certain characteristics of RISC and vice versa. Thus, the features of CISC and RISC architectures have been morphing with each other. Classic CISC architecture examples include VAX (by DEC), PDP-11 (by DEC), Motorola 68000 (by Freescale/Motorola), and x86 (mainly by Intel). The modern CISC architecture, x86-64, based processors like Pentium (by Intel) and Athlon (by AMD) basically evolved from the classic CISC architecture x86, but they exhibit several RISC features. Currently, Xeon (by Intel) and Opteron (by AMD) are the two quite prominent market icons based on x86-64 architecture. Famous RISC architecture examples include MIPS (by MIPS Technologies), POWER (mainly by IBM), SPARC (mainly by SUN/Oracle), ALPHA (by DEC), and ARM for embedded systems (by ARM Ltd.)."}]},{"#name":"para","$":{"view":"all","id":"p0040"},"$$":[{"#name":"__text__","_":"Today, Intel and AMD are two major vendors in the microprocessor industry, each with their own line of CPU architectures. The x86-64 CPUs from Intel and AMD, basically emerged as CISC architectures, now incorporate a number of RISC features, especially to provide for "},{"#name":"bold","_":"Instructions Level Parallelism"},{"#name":"__text__","_":"—"},{"#name":"bold","_":"ILP"},{"#name":"__text__","_":" (details later on). Interestingly, today the microprocessors (from Intel and AMD) implement the RISC feature of separate memory space for the data and the instructions (for Level-1 cache, at least)."}]},{"#name":"para","$":{"view":"all","id":"p0045"},"$$":[{"#name":"__text__","_":"Another main specialty of a modern CPU is that a number of CPU cores are fused together on a single chip/die with a common integrated memory controller for all the cores. Initially, dual core CPU chips were introduced around the year 2005 but, as of the year 2013, 12/16-core CPU chips are commonly available in the market, although the price might get manifold with linear increase in the number of cores per chip. Moreover, getting the best performance out of a larger number of cores in a single CPU chip is currently a challenging task, mainly due to the memory bandwidth limitations. A multicore CPU provides for more clock cycles by summing the clock cycles contributed by each of its cores. Thus, it is keeping the well-known "},{"#name":"italic","_":"Moore’s law"},{"#name":"__text__","_":" effective, even today, to some extent. In fact, they provide for tackling the issues of high power requirements and heat dissipation realized in the case when all the cores are there in separate CPU chips, instead of "},{"#name":"anchor","$":{"id":"p93"}},{"#name":"__text__","_":"being part of a single CPU chip "},{"#name":"cross-ref","$":{"id":"c0025","refid":"b0020"},"_":"[4]"},{"#name":"__text__","_":". Increment in the clock frequency of a single CPU core (silicon based) is virtually no more feasible due to the physical and practical obstacles. Multicore technology is the posed and accepted solution to this limitation."}]},{"#name":"para","$":{"view":"all","id":"p0050"},"$$":[{"#name":"__text__","_":"Another sophisticated architectural innovation in several modern CPU architectures is the multithreading facility per CPU core. A physical core acts as to provide more than one (usually two) "},{"#name":"italic","_":"logical processors"},{"#name":"__text__","_":" that might be benefited by the application in hand. The common realizations of this concept include "},{"#name":"italic","_":"hyperthreading"},{"#name":"__text__","_":", "},{"#name":"italic","_":"symmetric multithreading"},{"#name":"__text__","_":" (SMT), and "},{"#name":"italic","_":"chip multithreading"},{"#name":"__text__","_":" (CMT). A concise introduction to this topic and to the overall features of modern processors is given by Hager and Wellein ("},{"#name":"cross-ref","$":{"id":"c0030","refid":"b0025"},"_":"[5]"},{"#name":"__text__","_":", 1–36). Implications of several of the architectural features of the modern processors (especially multicore, multithreading, and ILP) are discussed in the coming sections."}]}]},"pii":"B9780124080898000033","isbn":"9780124080898","issn":"00652458","version":"S300.1","contentType":"BS","cid":"277250","title":"Advances in Computers","displayName":"Advances in Computers","coverDateYear":"2013","authors":{"#name":"rootwrapper","$":{"xmlns:ce":""},"$$":[{"#name":"author-group","$":{"id":"ag005"},"$$":[{"#name":"author","$":{"biographyid":"bg005","id":"au005"},"$$":[{"#name":"given-name","_":"Amjad"},{"#name":"surname","_":"Ali"},{"#name":"e-address","$":{"id":"em005","type":"email"},"_":"amjadali@bzu.edu.pk"}]},{"#name":"author","$":{"biographyid":"bg010","id":"au010"},"$$":[{"#name":"given-name","_":"Khalid Saifullah"},{"#name":"surname","_":"Syed"},{"#name":"e-address","$":{"id":"em010","type":"email"},"_":"khalidsaifullah@bzu.edu.pk"}]},{"#name":"affiliation","$":{"id":"af005"},"$$":[{"#name":"textfn","_":"Center for Advanced Studies in Pure and Applied Mathematics (CASPAM), Bahauddin Zakariya University (BZU), Multan, 60800, Pakistan"},{"#name":"affiliation","$":{"xmlns:sa":""},"$$":[{"#name":"organization","_":"Center for Advanced Studies in Pure and Applied Mathematics (CASPAM), Bahauddin Zakariya University (BZU)"},{"#name":"state","_":"Multan"},{"#name":"postal-code","_":"60800"},{"#name":"country","_":"Pakistan"}]}]}]}]},"floats":[],"attachments":[],"snippetMetadata":{"identifier":"B9780124080898000033","xpath":"//ce:section[@id='s0010']","version":"S300.1"}},{"body":{"#name":"section","$":{"view":"all","id":"s0110","xmlns:ce":""},"$$":[{"#name":"section-title","$":{"id":"st0090"},"_":"Program Store, Data Store"},{"#name":"para","$":{"view":"all","id":"p0270"},"$$":[{"#name":"__text__","_":"The "},{"#name":"italic","_":"program store"},{"#name":"__text__","_":" stores the instructions used in implementing the required DSP algorithms. In a general-purpose computer (von Neumann architecture), data and instructions are stored together. In most DSP systems, the program is stored separately from the data, since this allows faster execution of the instructions. Data can be moved on its own bus at the same time that instructions are being fetched. This architecture was developed from basic research performed at Harvard University, and therefore is generally called a "},{"#name":"italic","_":"Harvard architecture."},{"#name":"__text__","_":" Often the data bus and the instruction bus have different widths."},{"#name":"display","$$":[{"#name":"textbox","$":{"id":"b0030"},"$$":[{"#name":"textbox-head","$$":[{"#name":"title","_":"Insider Info"}]},{"#name":"textbox-body","$$":[{"#name":"sections","$$":[{"#name":"para","$":{"view":"all","id":"p0280"},"$$":[{"#name":"italic","_":"Quite often, three system buses can be found on DSP systems, one for instructions, one for data (including I/O) and one for transferring coefficients from a separate memory area or chip"},{"#name":"__text__","_":"."}]}]}]}]}]}]}]},"pii":"B9780750689762000031","isbn":"9780750689762","issn":null,"version":"S300.4","contentType":"BK","cid":"277395","title":"DSP System General Model","displayName":"Digital Signal Processing","coverDateYear":"2009","authors":{"#name":"rootwrapper","$":{"xmlns:ce":""},"$$":[{"#name":"author-group","$":{"xfab-added":"true"},"$$":[{"#name":"author","$$":[{"#name":"given-name","_":"James D."},{"#name":"surname","_":"Broesch"}]}]}]},"floats":[],"attachments":[],"snippetMetadata":{"identifier":"B9780750689762000031","xpath":"//ce:section[@id='s0110']","version":"S300.4"}}],"topicId":"14_493845063","definitionSourceTitle":null,"definitionSourcePii":null,"definitionSourceXPath":null,"definitionSourceYear":null,"topicUrlPath":"/computer-science/von-neumann-architecture","relatedConcepts":[{"parentDomain":"Physical Sciences and Engineering","childDomain":"Computer Science","topicName":"Supercomputer","topicURLPath":"/computer-science/supercomputer","topicId":"14_249601909"},{"parentDomain":"Physical Sciences and Engineering","childDomain":"Computer Science","topicName":"Computer Architecture","topicURLPath":"/computer-science/computer-architecture","topicId":"14_492444610"},{"parentDomain":"Physical Sciences and Engineering","childDomain":"Computer Science","topicName":"Harvard Architecture","topicURLPath":"/computer-science/harvard-architecture","topicId":"14_493435252"},{"parentDomain":"Physical Sciences and Engineering","childDomain":"Computer Science","topicName":"Microprocessor","topicURLPath":"/computer-science/microprocessor","topicId":"14_493436479"}],"piiList":["B9780128024591000063","B9780128150719000142","B9780128498903000034","B9780123742605000026","B9780750689762000080","B9780128053874000029","B9780123725127000080","B9780124080898000033","B9780750689762000031"]},"pdfDownload":{},"renderTarget":"html","alerts":{"showAlertsModal":false},"entitlements":{"B9780128024591000063":false,"B9780128150719000142":false,"B9780128498903000034":false,"B9780123742605000026":false,"B9780750689762000080":false,"B9780128053874000029":false,"B9780123725127000080":false,"B9780124080898000033":false,"B9780750689762000031":false}};
                        </script></div>
<script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/10/js/babel-polyfill/6.26.0/babel-polyfill.min.js" type="b921dd7e5908c3e856e29bb6-text/javascript"></script>
<script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/26/js/react/16.8.4/react.production.min.js" type="b921dd7e5908c3e856e29bb6-text/javascript"></script>
<script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/26/js/react-dom/16.8.4/react-dom.production.min.js" type="b921dd7e5908c3e856e29bb6-text/javascript"></script>
<script type="b921dd7e5908c3e856e29bb6-text/javascript">(function(){
    if (document.querySelector('math')) {
      var hasMathML = false;
      if (document.createElementNS) {
        var ns = 'http://www.w3.org/1998/Math/MathML', div = document.createElement('div');
        div.style.position = 'absolute';
        var mfrac = div.appendChild(document.createElementNS(ns, 'math')).appendChild(document.createElementNS(ns, 'mfrac'));
        mfrac.appendChild(document.createElementNS(ns, 'mi')).appendChild(document.createTextNode('xx'));
        mfrac.appendChild(document.createElementNS(ns, 'mi')).appendChild(document.createTextNode('yy'));
        document.body.appendChild(div);
        hasMathML = div.offsetHeight > div.offsetWidth;
        document.body.removeChild(div);
      }
      if (!hasMathML) {
        var script = document.createElement('script');
        script.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=MML_SVG';
        document.body.appendChild(script);
      }
    }
  })()</script>
<script type="b921dd7e5908c3e856e29bb6-text/javascript">
      window.isIE11 = !!window.MSInputMethodContext && !!document.documentMode;
    </script>
<script type="b921dd7e5908c3e856e29bb6-text/javascript">
        window.assetsBaseURL = 'https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/78869a99bb515d27b0258c9bf222cd4d21e6a37f';
        window.pageData = {"page":{"businessUnit":"els:rp:st","language":"en","name":"foundationalcontent:topics:topicdetail","noTracking":"false","productName":"sd","type":"cp-cr","environment":"prod","loadTimestamp":"1574219018100"},"content":[{"title":"computer-science ^ von-neumann-architecture","format":"MIME-XHTML","id":"sd:topic:14_493845063","type":"sd:topic"}],"visitor":{"accessType":"ae:ANON_GUEST","accountId":"ae:228598","accountName":"ae:ScienceDirect Guests","userId":"ae:12975512","ipAddress":"202.142.178.42","appSessionId":"1a363fca-e9a5-476a-9b4d-fe9cd8cca1ef"}};
      </script>
<div id="dynamic-js-holder"></div>
<script src='https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/78869a99bb515d27b0258c9bf222cd4d21e6a37f/client.js' async type="b921dd7e5908c3e856e29bb6-text/javascript"></script>

<script type="b921dd7e5908c3e856e29bb6-text/javascript">
        window.lightningjs||function(c){function g(b,d){d&&(d+=(/\?/.test(d)?"&":"?")+"lv=1");c[b]||function(){var i=window,h=document,j=b,g=h.location.protocol,l="load",k=0;(function(){function b(){a.P(l);a.w=1;c[j]("_load")}c[j]=function(){function m(){m.id=e;return c[j].apply(m,arguments)}var b,e=++k;b=this&&this!=i?this.id||0:0;(a.s=a.s||[]).push([e,b,arguments]);m.then=function(b,c,h){var d=a.fh[e]=a.fh[e]||[],j=a.eh[e]=a.eh[e]||[],f=a.ph[e]=a.ph[e]||[];b&&d.push(b);c&&j.push(c);h&&f.push(h);return m};return m};var a=c[j]._={};a.fh={};a.eh={};a.ph={};a.l=d?d.replace(/^\/\//,(g=="https:"?g:"http:")+"//"):d;a.p={0:+new Date};a.P=function(b){a.p[b]=new Date-a.p[0]};a.w&&b();i.addEventListener?i.addEventListener(l,b,!1):i.attachEvent("on"+l,b);var q=function(){function b(){return["<head></head><",c,' onload="var d=',n,";d.getElementsByTagName('head')[0].",d,"(d.",g,"('script')).",i,"='",a.l,"'\"></",c,">"].join("")}var c="body",e=h[c];if(!e)return setTimeout(q,100);a.P(1);var d="appendChild",g="createElement",i="src",k=h[g]("div"),l=k[d](h[g]("div")),f=h[g]("iframe"),n="document",p;k.style.display="none";e.insertBefore(k,e.firstChild).id=o+"-"+j;f.frameBorder="0";f.id=o+"-frame-"+j;/MSIE[ ]+6/.test(navigator.userAgent)&&(f[i]="javascript:false");f.allowTransparency="true";l[d](f);try{f.contentWindow[n].open()}catch(s){a.domain=h.domain,p="javascript:var d="+n+".open();d.domain='"+h.domain+"';",f[i]=p+"void(0);"}try{var r=f.contentWindow[n];r.write(b());r.close()}catch(t){f[i]=p+'d.write("'+b().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};a.l&&setTimeout(q,0)})()}();c[b].lv="1";return c[b]}var o="lightningjs",k=window[o]=g(o);k.require=g;k.modules=c}({});
        window.usabilla_live = lightningjs.require("usabilla_live", "https://w.usabilla.com/4061dbc431ee.js");
        var customData = {};

        if(window.pageData && pageData.content && pageData.content[0]) {
          customData.entitlementType = pageData.content[0].entitlementType;
        }
        if(window.pageData && pageData.visitor) {
          customData.accessType = pageData.visitor.accessType;
          customData.accountId = pageData.visitor.accountId;
          customData.loginStatus = pageData.visitor.loginStatus;
        }
        usabilla_live("data", {"custom": customData });
      </script>

<script type="b921dd7e5908c3e856e29bb6-text/javascript">
        function getPageLoadTime() {
          var returnValue = '';
          if (typeof(performance) !== 'undefined' && typeof(performance.timing) == 'object') {
              var timing = performance.timing;
              var startTime = timing.redirectStart || timing.fetchStart || timing.requestStart;
              var endTime = timing.domContentLoadedEventEnd || timing.domInteractive || timing.domComplete || timing.loadEventComplete;
              if (startTime && endTime && startTime < endTime) returnValue = endTime - startTime;
          }
          return returnValue.toString();
        }
        setTimeout(function() {
          window.pageData.page.loadTime = getPageLoadTime();
        }, 0);
        try { pageDataTracker.trackPageLoad(); }
        catch(e) { console.warn("There was an error loading or running Adobe DTM: ", e); }
      </script>
<script src="https://ajax.cloudflare.com/cdn-cgi/scripts/95c75768/cloudflare-static/rocket-loader.min.js" data-cf-settings="b921dd7e5908c3e856e29bb6-|49" defer=""></script></body>
</html>